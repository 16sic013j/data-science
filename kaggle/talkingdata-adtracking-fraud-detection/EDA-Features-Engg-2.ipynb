{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T10:23:05.422546Z",
     "start_time": "2018-04-22T10:23:03.176769Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib as mplt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.dates as mdates\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import math\n",
    "import gc\n",
    "import ipaddress\n",
    "from urllib.parse import urlparse\n",
    "from tldextract import extract\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from data_science_utils import dataframe as df_utils\n",
    "from data_science_utils import models as model_utils\n",
    "from data_science_utils.dataframe import column as column_utils\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (24,4)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "import missingno as msno\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "import datetime\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T10:25:47.923219Z",
     "start_time": "2018-04-22T10:23:05.424759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_hdf(\"data/train_1.h5\",\"table\")\n",
    "df_test = pd.read_hdf(\"data/test_1.h5\",\"table\")\n",
    "df_pos = df_train[df_train['is_attributed']==1]\n",
    "df_negs = df_train[df_train['is_attributed']==0]\n",
    "df_negs = df_negs.sample(frac=0.7)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T10:26:03.218471Z",
     "start_time": "2018-04-22T10:25:47.925253Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129569777, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "        \n",
    "\n",
    "df_train = pd.concat([df_pos,df_negs],ignore_index=True)\n",
    "df_train.shape\n",
    "df_pos = None\n",
    "df_negs = None\n",
    "del df_pos\n",
    "del df_negs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T10:26:03.336676Z",
     "start_time": "2018-04-22T10:26:03.224384Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_dataframe(df,splits=[0.6,0.4]):\n",
    "    if(abs(np.sum(splits)-1)>1e-9):\n",
    "        raise ValueError(\"sum of splits should be 1\")\n",
    "    probs = np.random.rand(len(df))\n",
    "    lower = 0\n",
    "    upper = 0\n",
    "    dfs = list()\n",
    "    for i in splits:\n",
    "        upper = lower+i\n",
    "        mask = (probs>=lower) & (probs < upper)\n",
    "        print(\"lower: %s, upper: %s mask len: %s\"%(lower,upper,np.sum(mask)))\n",
    "        df_new = df[mask]\n",
    "        dfs.append(df_new)\n",
    "        lower = upper\n",
    "    return dfs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T10:26:35.350528Z",
     "start_time": "2018-04-22T10:26:03.338945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower: 0, upper: 0.6 mask len: 77746339\n",
      "lower: 0.6, upper: 1.0 mask len: 51823438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = split_dataframe(df_train)\n",
    "df_train = splits[0]\n",
    "df_cv1 = splits[1]\n",
    "#df_cv2 = splits[2]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T10:26:35.436362Z",
     "start_time": "2018-04-22T10:26:35.352491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T10:26:35.482931Z",
     "start_time": "2018-04-22T10:26:35.438211Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define all the groupby transformations\n",
    "GROUPBY_AGGREGATIONS = [\n",
    "    \n",
    "    # Frequency of is_attributed in different groups\n",
    "    {'groupby': ['ip'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['app'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['device'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['os'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['channel'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['hour'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['ip','app'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['ip','hour'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['ip','device'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['ip','channel'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['app','device'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['app','hour'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['app','os'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['app','channel'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['os','channel'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['os','hour'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['device','channel'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['channel','hour'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['device','hour'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['ip','app','device'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['ip','app','channel'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['app','device','os'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['app','channel','os'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['ip','os','channel'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['ip','os','hour'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['ip','app','hour'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['app','os','hour'], 'select': 'is_attributed', 'agg': 'mean'}\n",
    "    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T10:26:35.490756Z",
     "start_time": "2018-04-22T10:26:35.484994Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define all the groupby transformations\n",
    "GROUPBY_AGGREGATIONS_v2 = [\n",
    "    \n",
    "    # Frequency of is_attributed in different groups\n",
    "    {'groupby': ['ip','device','hour'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['ip','channel','hour'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['device','channel','hour'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['app','channel','hour'], 'select': 'is_attributed', 'agg': 'mean'},\n",
    "    {'groupby': ['os','channel','hour'], 'select': 'is_attributed', 'agg': 'mean'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T11:24:11.489128Z",
     "start_time": "2018-04-22T11:24:11.381942Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_groupby_specs(GROUPBY_AGGREGATIONS):\n",
    "    gc.collect()\n",
    "    \n",
    "    for spec in GROUPBY_AGGREGATIONS:\n",
    "        power = 5-len(spec['groupby'])\n",
    "        log_group = np.log(pow(10,power))\n",
    "        min_examples_for_validity = 5\n",
    "        # Name of the aggregation we're applying\n",
    "        agg_name = spec['agg']\n",
    "\n",
    "        # Info\n",
    "        print(\"Grouping by {}, and aggregating {} with {}\".format(\n",
    "            spec['groupby'], spec['select'], agg_name\n",
    "        ))\n",
    "\n",
    "        # Unique list of features to select\n",
    "        all_features = list(set(spec['groupby'] + [spec['select']]))\n",
    "\n",
    "        # Name of new feature\n",
    "        new_feature = '{}_{}_{}'.format('_'.join(spec['groupby']), agg_name, spec['select'])\n",
    "\n",
    "        # Perform the groupby\n",
    "        group = df_train[all_features].groupby(spec['groupby'])[spec['select']]\n",
    "        \n",
    "        gp = group.transform(spec['agg'])\n",
    "        gp_count = group.transform('count')\n",
    "        c1 = gp_count<=min_examples_for_validity\n",
    "        gp_count[c1] = np.nan\n",
    "        \n",
    "        \n",
    "        gp[c1] = np.nan\n",
    "        \n",
    "        \n",
    "        conf = np.log1p(gp_count) / log_group\n",
    "        # conf[conf<0.15] = 0.15\n",
    "        gp = gp*conf\n",
    "        df_train[new_feature] = gp\n",
    "        gp = None\n",
    "        del gp\n",
    "        \n",
    "        \n",
    "#         gpt = group.agg({'mean':np.mean,'count':lambda x: x.shape[0]}).reset_index()\n",
    "#         gpt['conf'] = np.log1p(gpt['count']) / log_group\n",
    "#         gpt[new_feature] = gpt['mean']*gpt['conf']\n",
    "        \n",
    "        new_feature_count = new_feature+'_count'\n",
    "        gpt = group.agg(spec['agg']).reset_index().rename(index=str, columns={spec['select']: new_feature})\n",
    "        gpt_count = group.agg('count').reset_index().rename(index=str, columns={spec['select']: new_feature_count})[new_feature_count]\n",
    "        \n",
    "        c1 = gpt_count<=min_examples_for_validity\n",
    "        \n",
    "\n",
    "        gpt_count[c1] = np.nan\n",
    "        \n",
    "        gptfn=gpt[new_feature]\n",
    "        gptfn[c1] = np.nan\n",
    "        gpt[new_feature] = gptfn\n",
    "\n",
    "        conf = np.log1p(gpt_count) / log_group\n",
    "        # conf[conf<0.15] = 0.15\n",
    "        gpt[new_feature] = gpt[new_feature]*conf\n",
    "        conf = None\n",
    "        \n",
    "        gp_count = None\n",
    "        group = None\n",
    "        gpt_count = None\n",
    "        del group\n",
    "\n",
    "        # Merge back to X_train\n",
    "        # df_train[new_feature] = df_train[spec['groupby']].merge(gpt, on=spec['groupby'], how='left')[new_feature]\n",
    "        df_test[new_feature] = pd.merge(df_test[spec['groupby']],gpt,how='left',on=spec['groupby'])[new_feature].values\n",
    "        df_cv1[new_feature] = pd.merge(df_cv1[spec['groupby']],gpt,how='left',on=spec['groupby'])[new_feature].values\n",
    "\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T11:24:12.626951Z",
     "start_time": "2018-04-22T11:24:12.579774Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T11:24:13.628858Z",
     "start_time": "2018-04-22T11:24:13.626024Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# df_train=df_train[df_train['ip']==53672]\n",
    "# df_cv1=df_cv1[df_cv1['ip']==53672]\n",
    "# df_test=df_test[df_test['ip']==53672]\n",
    "# add_groupby_specs([{'groupby': ['ip'], 'select': 'is_attributed', 'agg': 'mean'}])\n",
    "\n",
    "\n",
    "# df_train[df_train['ip']==53672].shape\n",
    "\n",
    "# df_train[(df_train['ip']==53672)&(df_train['is_attributed']==1)].shape\n",
    "\n",
    "# df_cv1[df_cv1['ip']==53672].shape\n",
    "\n",
    "# df_cv1[(df_cv1['ip']==53672)&(df_cv1['is_attributed']==1)].shape\n",
    "\n",
    "# df_train[df_train['ip']==53672].sample(10)\n",
    "\n",
    "\n",
    "# df_cv1[df_cv1['ip']==53672].sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T12:03:55.952794Z",
     "start_time": "2018-04-22T11:24:14.752475Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping by ['ip'], and aggregating is_attributed with mean\n",
      "Grouping by ['app'], and aggregating is_attributed with mean\n",
      "Grouping by ['device'], and aggregating is_attributed with mean\n",
      "Grouping by ['os'], and aggregating is_attributed with mean\n",
      "Grouping by ['channel'], and aggregating is_attributed with mean\n",
      "Grouping by ['hour'], and aggregating is_attributed with mean\n",
      "Grouping by ['ip', 'app'], and aggregating is_attributed with mean\n",
      "Grouping by ['ip', 'hour'], and aggregating is_attributed with mean\n",
      "Grouping by ['ip', 'device'], and aggregating is_attributed with mean\n",
      "Grouping by ['ip', 'channel'], and aggregating is_attributed with mean\n",
      "Grouping by ['app', 'device'], and aggregating is_attributed with mean\n",
      "Grouping by ['app', 'hour'], and aggregating is_attributed with mean\n",
      "Grouping by ['app', 'os'], and aggregating is_attributed with mean\n",
      "Grouping by ['app', 'channel'], and aggregating is_attributed with mean\n",
      "Grouping by ['os', 'channel'], and aggregating is_attributed with mean\n",
      "Grouping by ['os', 'hour'], and aggregating is_attributed with mean\n",
      "Grouping by ['device', 'channel'], and aggregating is_attributed with mean\n",
      "Grouping by ['channel', 'hour'], and aggregating is_attributed with mean\n",
      "Grouping by ['device', 'hour'], and aggregating is_attributed with mean\n",
      "Grouping by ['ip', 'app', 'device'], and aggregating is_attributed with mean\n",
      "Grouping by ['ip', 'app', 'channel'], and aggregating is_attributed with mean\n",
      "Grouping by ['app', 'device', 'os'], and aggregating is_attributed with mean\n",
      "Grouping by ['app', 'channel', 'os'], and aggregating is_attributed with mean\n",
      "Grouping by ['ip', 'os', 'channel'], and aggregating is_attributed with mean\n",
      "Grouping by ['ip', 'os', 'hour'], and aggregating is_attributed with mean\n",
      "Grouping by ['ip', 'app', 'hour'], and aggregating is_attributed with mean\n",
      "Grouping by ['app', 'os', 'hour'], and aggregating is_attributed with mean\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping by ['ip', 'device', 'hour'], and aggregating is_attributed with mean\n",
      "Grouping by ['ip', 'channel', 'hour'], and aggregating is_attributed with mean\n",
      "Grouping by ['device', 'channel', 'hour'], and aggregating is_attributed with mean\n",
      "Grouping by ['app', 'channel', 'hour'], and aggregating is_attributed with mean\n",
      "Grouping by ['os', 'channel', 'hour'], and aggregating is_attributed with mean\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "add_groupby_specs(GROUPBY_AGGREGATIONS)\n",
    "gc.collect()\n",
    "add_groupby_specs(GROUPBY_AGGREGATIONS_v2)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T12:03:56.101813Z",
     "start_time": "2018-04-22T12:03:55.953885Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define all the groupby transformations\n",
    "cols = ['ip','app','device','os','channel','hour']\n",
    "GROUPBY_AGGREGATIONS_NO_SCALING = [\n",
    "    \n",
    "    \n",
    "    # V1 - GroupBy Features #\n",
    "    #########################    \n",
    "\n",
    "    # Variance in hour, for ip-app-os\n",
    "    {'groupby': ['ip','app','os'], 'select': 'hour', 'agg': 'var'},\n",
    "    # Variance in hour, for ip-day-channel\n",
    "    {'groupby': ['ip','channel'], 'select': 'hour', 'agg': 'var'},\n",
    "    \n",
    "    \n",
    "    \n",
    "    {'groupby': ['app','device'], 'select': 'hour', 'agg': 'var'},\n",
    "    \n",
    "    {'groupby': ['device','os'], 'select': 'hour', 'agg': 'var'},\n",
    "    \n",
    "    {'groupby': ['device'], 'select': 'hour', 'agg': 'var'},\n",
    "    \n",
    "    {'groupby': ['ip'], 'select': 'hour', 'agg': 'var'},\n",
    "    \n",
    "    \n",
    "\n",
    "    {'groupby': ['ip','hour'], 'select': 'channel', 'agg': 'count'},\n",
    "    # Count, for ip-app\n",
    "    {'groupby': ['ip', 'app'], 'select': 'channel', 'agg': 'count'},        \n",
    "    # Count, for ip-app-os\n",
    "    {'groupby': ['ip', 'app', 'os'], 'select': 'channel', 'agg': 'count'},\n",
    "    # Count, for ip-app-hour\n",
    "    {'groupby': ['ip','app','hour'], 'select': 'channel', 'agg': 'count'},\n",
    "    \n",
    "    {'groupby': ['app','hour'], 'select': 'channel', 'agg': 'count'},\n",
    "    \n",
    "    {'groupby': ['device','hour'], 'select': 'channel', 'agg': 'count'},\n",
    "    \n",
    "    {'groupby': ['device'], 'select': 'channel', 'agg': 'count'},\n",
    "    \n",
    "    {'groupby': ['os','hour'], 'select': 'channel', 'agg': 'count'},\n",
    "    # Mean hour, for ip-app-channel\n",
    "    {'groupby': ['ip','app','channel'], 'select': 'hour', 'agg': 'mean'},\n",
    "    \n",
    "    {'groupby': ['app','channel'], 'select': 'hour', 'agg': 'mean'},\n",
    "    \n",
    "    {'groupby': ['ip'], 'select': 'hour', 'agg': 'mean'},\n",
    "    \n",
    "    {'groupby': ['device','app'], 'select': 'hour', 'agg': 'mean'},\n",
    "    \n",
    "    {'groupby': ['device','os'], 'select': 'hour', 'agg': 'mean'},\n",
    "    \n",
    "    # V2 - GroupBy Features #\n",
    "    #########################\n",
    "    # Average clicks on app by distinct users; is it an app they return to?\n",
    "    {'groupby': ['app'], \n",
    "     'select': 'ip', \n",
    "     'agg': lambda x: float(len(x)) / len(x.unique()), \n",
    "     'agg_name': 'AvgViewPerDistinct'\n",
    "    },\n",
    "    # How popular is the app or channel?\n",
    "    {'groupby': ['app'], 'select': 'channel', 'agg': 'count'},\n",
    "    {'groupby': ['channel'], 'select': 'app', 'agg': 'count'},\n",
    "    \n",
    "    {'groupby': ['ip'], 'select': 'channel', 'agg': 'count'},\n",
    "    {'groupby': ['device'], 'select': 'app', 'agg': 'count'},\n",
    "    \n",
    "    {'groupby': ['device'], 'select': 'os', 'agg': 'count'},\n",
    "    \n",
    "    {'groupby': ['os'], 'select': 'app', 'agg': 'count'},\n",
    "    \n",
    "    {'groupby': ['app'], 'select': 'os', 'agg': 'count'},\n",
    "    \n",
    "    # V3 - GroupBy Features                                              #\n",
    "    # https://www.kaggle.com/bk0000/non-blending-lightgbm-model-lb-0-977 #\n",
    "    ###################################################################### \n",
    "    {'groupby': ['ip'], 'select': 'channel', 'agg': 'nunique'}, \n",
    "    {'groupby': ['ip'], 'select': 'app', 'agg': 'nunique'}, \n",
    "\n",
    "    {'groupby': ['ip','app'], 'select': 'os', 'agg': 'nunique'}, \n",
    "    {'groupby': ['ip'], 'select': 'device', 'agg': 'nunique'}, \n",
    "    {'groupby': ['app'], 'select': 'channel', 'agg': 'nunique'}, \n",
    "    {'groupby': ['ip', 'device', 'os'], 'select': 'app', 'agg': 'nunique'},\n",
    "    \n",
    "    {'groupby': ['channel'], 'select': 'ip', 'agg': 'nunique'}, \n",
    "    {'groupby': ['app'], 'select': 'ip', 'agg': 'nunique'}, \n",
    "    {'groupby': ['hour'], 'select': 'ip', 'agg': 'nunique'}, \n",
    "    {'groupby': ['os','app'], 'select': 'ip', 'agg': 'nunique'}, \n",
    "    {'groupby': ['device'], 'select': 'ip', 'agg': 'nunique'}, \n",
    "    {'groupby': ['channel'], 'select': 'app', 'agg': 'nunique'}, \n",
    "    {'groupby': ['app', 'device', 'os'], 'select': 'ip', 'agg': 'nunique'},\n",
    "    \n",
    "    {'groupby': ['channel'], 'select': 'os', 'agg': 'nunique'}, \n",
    "    {'groupby': ['app'], 'select': 'channel', 'agg': 'nunique'},\n",
    "    {'groupby': ['app'], 'select': 'device', 'agg': 'nunique'},\n",
    "    {'groupby': ['hour'], 'select': 'device', 'agg': 'nunique'}, \n",
    "    {'groupby': ['os','app'], 'select': 'device', 'agg': 'nunique'}, \n",
    "    {'groupby': ['device'], 'select': 'channel', 'agg': 'nunique'}, \n",
    "    \n",
    "    \n",
    "\n",
    "    {'groupby': ['device'], 'select': 'app', 'agg': 'nunique'},\n",
    "    \n",
    "    {'groupby': ['device'], 'select': 'os', 'agg': 'nunique'},\n",
    "    \n",
    "    {'groupby': ['device'], 'select': 'hour', 'agg': 'nunique'},\n",
    "    \n",
    "    {'groupby': ['os'], 'select': 'app', 'agg': 'nunique'},\n",
    "    \n",
    "    {'groupby': ['app'], 'select': 'os', 'agg': 'nunique'},\n",
    "    \n",
    "    {'groupby': ['hour'], 'select': 'os', 'agg': 'nunique'},\n",
    "    {'groupby': ['hour'], 'select': 'channel', 'agg': 'nunique'},\n",
    "    \n",
    "    {'groupby': ['hour'], 'select': 'app', 'agg': 'nunique'}\n",
    "    \n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T12:03:56.168464Z",
     "start_time": "2018-04-22T12:03:56.102882Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_groupby_specs_no_scaling(GROUPBY_AGGREGATIONS):\n",
    "    gc.collect()\n",
    "    \n",
    "    for spec in GROUPBY_AGGREGATIONS:\n",
    "        min_examples_for_validity = 5\n",
    "        # Name of the aggregation we're applying\n",
    "        agg_name = spec['agg_name'] if 'agg_name' in spec else spec['agg']\n",
    "\n",
    "        # Info\n",
    "        print(\"Grouping by {}, and aggregating {} with {}\".format(\n",
    "            spec['groupby'], spec['select'], agg_name\n",
    "        ))\n",
    "\n",
    "        # Unique list of features to select\n",
    "        all_features = list(set(spec['groupby'] + [spec['select']]))\n",
    "\n",
    "        # Name of new feature\n",
    "        new_feature = '{}_{}_{}'.format('_'.join(spec['groupby']), agg_name, spec['select'])\n",
    "\n",
    "        # Perform the groupby\n",
    "        group = df_train[all_features].groupby(spec['groupby'])[spec['select']]\n",
    "        \n",
    "        gp = group.transform(spec['agg'])\n",
    "        #gp_count = group.transform('count')\n",
    "        #c1 = gp_count<=min_examples_for_validity\n",
    "        #gp_count[c1] = np.nan\n",
    "        \n",
    "        \n",
    "        #gp[c1] = np.nan\n",
    "        \n",
    "        \n",
    "        #conf = np.log1p(gp_count) / log_group\n",
    "        # conf[conf<0.15] = 0.15\n",
    "        #gp = gp*conf\n",
    "        df_train[new_feature] = gp\n",
    "        gp = None\n",
    "        del gp\n",
    "        \n",
    "        \n",
    "        new_feature_count = new_feature+'_count'\n",
    "        gpt = group.agg(spec['agg']).reset_index().rename(index=str, columns={spec['select']: new_feature})\n",
    "        #gpt_count = group.agg('count').reset_index().rename(index=str, columns={spec['select']: new_feature_count})[new_feature_count]\n",
    "        #c1 = gpt_count<=min_examples_for_validity\n",
    "        #gpt_count[c1] = np.nan\n",
    "        #gptfn=gpt[new_feature]\n",
    "        #gptfn[c1] = np.nan\n",
    "        #gpt[new_feature] = gptfn\n",
    "\n",
    "        #conf = np.log1p(gpt_count) / log_group\n",
    "        # conf[conf<0.15] = 0.15\n",
    "        #gpt[new_feature] = gpt[new_feature]*conf\n",
    "        #conf = None\n",
    "        \n",
    "        #gp_count = None\n",
    "        group = None\n",
    "        #gpt_count = None\n",
    "        del group\n",
    "\n",
    "        # Merge back to X_train\n",
    "        # df_train[new_feature] = df_train[spec['groupby']].merge(gpt, on=spec['groupby'], how='left')[new_feature]\n",
    "        df_test[new_feature] = pd.merge(df_test[spec['groupby']],gpt,how='left',on=spec['groupby'])[new_feature].values\n",
    "        df_cv1[new_feature] = pd.merge(df_cv1[spec['groupby']],gpt,how='left',on=spec['groupby'])[new_feature].values\n",
    "\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T15:05:12.517127Z",
     "start_time": "2018-04-22T12:03:56.169446Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping by ['ip', 'app', 'os'], and aggregating hour with var\n",
      "Grouping by ['ip', 'channel'], and aggregating hour with var\n",
      "Grouping by ['app', 'device'], and aggregating hour with var\n",
      "Grouping by ['device', 'os'], and aggregating hour with var\n",
      "Grouping by ['device'], and aggregating hour with var\n",
      "Grouping by ['ip'], and aggregating hour with var\n",
      "Grouping by ['ip', 'hour'], and aggregating channel with count\n",
      "Grouping by ['ip', 'app'], and aggregating channel with count\n",
      "Grouping by ['ip', 'app', 'os'], and aggregating channel with count\n",
      "Grouping by ['ip', 'app', 'hour'], and aggregating channel with count\n",
      "Grouping by ['app', 'hour'], and aggregating channel with count\n",
      "Grouping by ['device', 'hour'], and aggregating channel with count\n",
      "Grouping by ['device'], and aggregating channel with count\n",
      "Grouping by ['os', 'hour'], and aggregating channel with count\n",
      "Grouping by ['ip', 'app', 'channel'], and aggregating hour with mean\n",
      "Grouping by ['app', 'channel'], and aggregating hour with mean\n",
      "Grouping by ['ip'], and aggregating hour with mean\n",
      "Grouping by ['device', 'app'], and aggregating hour with mean\n",
      "Grouping by ['device', 'os'], and aggregating hour with mean\n",
      "Grouping by ['app'], and aggregating ip with AvgViewPerDistinct\n",
      "Grouping by ['app'], and aggregating channel with count\n",
      "Grouping by ['channel'], and aggregating app with count\n",
      "Grouping by ['ip'], and aggregating channel with count\n",
      "Grouping by ['device'], and aggregating app with count\n",
      "Grouping by ['device'], and aggregating os with count\n",
      "Grouping by ['os'], and aggregating app with count\n",
      "Grouping by ['app'], and aggregating os with count\n",
      "Grouping by ['ip'], and aggregating channel with nunique\n",
      "Grouping by ['ip'], and aggregating app with nunique\n",
      "Grouping by ['ip', 'app'], and aggregating os with nunique\n",
      "Grouping by ['ip'], and aggregating device with nunique\n",
      "Grouping by ['app'], and aggregating channel with nunique\n",
      "Grouping by ['ip', 'device', 'os'], and aggregating app with nunique\n",
      "Grouping by ['channel'], and aggregating ip with nunique\n",
      "Grouping by ['app'], and aggregating ip with nunique\n",
      "Grouping by ['hour'], and aggregating ip with nunique\n",
      "Grouping by ['os', 'app'], and aggregating ip with nunique\n",
      "Grouping by ['device'], and aggregating ip with nunique\n",
      "Grouping by ['channel'], and aggregating app with nunique\n",
      "Grouping by ['app', 'device', 'os'], and aggregating ip with nunique\n",
      "Grouping by ['channel'], and aggregating os with nunique\n",
      "Grouping by ['app'], and aggregating channel with nunique\n",
      "Grouping by ['app'], and aggregating device with nunique\n",
      "Grouping by ['hour'], and aggregating device with nunique\n",
      "Grouping by ['os', 'app'], and aggregating device with nunique\n",
      "Grouping by ['device'], and aggregating channel with nunique\n",
      "Grouping by ['device'], and aggregating app with nunique\n",
      "Grouping by ['device'], and aggregating os with nunique\n",
      "Grouping by ['device'], and aggregating hour with nunique\n",
      "Grouping by ['os'], and aggregating app with nunique\n",
      "Grouping by ['app'], and aggregating os with nunique\n",
      "Grouping by ['hour'], and aggregating os with nunique\n",
      "Grouping by ['hour'], and aggregating channel with nunique\n",
      "Grouping by ['hour'], and aggregating app with nunique\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "add_groupby_specs_no_scaling(GROUPBY_AGGREGATIONS_NO_SCALING)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T15:09:50.895997Z",
     "start_time": "2018-04-22T15:05:12.518201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv1_2_2.h5 done\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "df_cv1.to_hdf('data/cv1_2.h5',\"table\",mode='w')\n",
    "gc.collect()\n",
    "print(\"cv1_2_2.h5 done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T16:12:03.619450Z",
     "start_time": "2018-04-22T16:12:03.617423Z"
    }
   },
   "outputs": [],
   "source": [
    "df_cv1 = None\n",
    "del df_cv1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "df_test.to_hdf('data/test_2.h5',\"table\",mode='w')\n",
    "gc.collect()\n",
    "print(\"Test_2.h5 done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T16:12:12.447520Z",
     "start_time": "2018-04-22T16:12:11.870425Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4270"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = None\n",
    "del df_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T16:31:55.876091Z",
     "start_time": "2018-04-22T16:29:20.405101Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_2.h5 done\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "df_train.to_hdf('data/train_2.h5',\"table\",mode='w')\n",
    "gc.collect()\n",
    "print(\"Train_2.h5 done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T16:09:36.942927Z",
     "start_time": "2018-04-22T16:07:52.255096Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2764"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_2.h5 done\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T15:11:03.162677Z",
     "start_time": "2018-04-22T11:24:40.789Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.groupby(['is_attributed']).mean()\n",
    "df_cv1.groupby(['is_attributed']).mean()\n",
    "# df_cv2.groupby(['is_attributed']).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T15:11:03.163212Z",
     "start_time": "2018-04-22T11:24:42.646Z"
    }
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T15:11:03.163731Z",
     "start_time": "2018-04-22T11:24:45.646Z"
    }
   },
   "outputs": [],
   "source": [
    "df_utils.count_nulls(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T21:33:59.493241Z",
     "start_time": "2018-04-20T21:33:59.440411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
