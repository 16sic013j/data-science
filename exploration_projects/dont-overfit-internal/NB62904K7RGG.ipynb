{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T19:50:46.833791Z",
     "start_time": "2019-03-19T19:50:36.801788Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ahemf/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/ahemf/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ahemf/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ahemf/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw to /Users/ahemf/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ahemf/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/ahemf/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ahemf/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ahemf/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw to /Users/ahemf/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<module 'data_science_utils.preprocessing' from '/Users/ahemf/anaconda3/lib/python3.7/site-packages/data_science_utils/preprocessing/__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<module 'data_science_utils.misc' from '/Users/ahemf/anaconda3/lib/python3.7/site-packages/data_science_utils/misc/__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ahemf/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/ahemf/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ahemf/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ahemf/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw to /Users/ahemf/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'data_science_utils.nlp' from '/Users/ahemf/anaconda3/lib/python3.7/site-packages/data_science_utils/nlp/__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<module 'data_science_utils.models' from '/Users/ahemf/anaconda3/lib/python3.7/site-packages/data_science_utils/models/__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import more_itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import multiprocessing\n",
    "\n",
    "pd.options.display.max_rows=900\n",
    "pd.options.display.max_columns=900\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from data_science_utils import dataframe as df_utils\n",
    "from data_science_utils import plots as plot_utils\n",
    "from data_science_utils.dataframe import column as column_utils\n",
    "from data_science_utils import misc as misc\n",
    "\n",
    "import warnings\n",
    "import traceback\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys, os\n",
    "import missingno as msno\n",
    "import random\n",
    "import gc\n",
    "\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import dill\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import more_itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn import pipeline\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from xgboost import XGBClassifier\n",
    "import multiprocessing\n",
    "\n",
    "pd.options.display.max_rows=900\n",
    "pd.options.display.max_columns=900\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "from data_science_utils import dataframe as df_utils\n",
    "from data_science_utils import models as model_utils\n",
    "from data_science_utils import plots as plot_utils\n",
    "from data_science_utils.dataframe import column as column_utils\n",
    "from data_science_utils import misc as misc\n",
    "from data_science_utils import preprocessing as pp_utils\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from data_science_utils.dataframe import get_specific_cols\n",
    "\n",
    "\n",
    "\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import BaseWrapper\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import TerminateOnNaN\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import BaseWrapper\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import optimizers\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import warnings\n",
    "import traceback\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "import sys, os\n",
    "import missingno as msno\n",
    "import random\n",
    "sys.path.append(os.getcwd())\n",
    "from fastnumbers import isfloat\n",
    "import gc\n",
    "\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import csv\n",
    "csv.field_size_limit()\n",
    "csv.field_size_limit(10000000)\n",
    "\n",
    "import re\n",
    "import ast\n",
    "from gensim import models, corpora\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.corpora import MmCorpus\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from nltk import bigrams\n",
    "from nltk import trigrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt');\n",
    "nltk.download('stopwords');\n",
    "nltk.download('wordnet');\n",
    "nltk.download('averaged_perceptron_tagger');\n",
    "nltk.download('omw');\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "stopwords_list = stopwords.words('english')\n",
    "import gc\n",
    "from data_science_utils import nlp as nlp_utils\n",
    "from importlib import reload\n",
    "reload(pp_utils)\n",
    "reload(misc)\n",
    "reload(nlp_utils)\n",
    "reload(model_utils)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T19:50:50.382038Z",
     "start_time": "2019-03-19T19:50:50.378730Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T20:18:22.416718Z",
     "start_time": "2019-03-19T20:18:16.386660Z"
    }
   },
   "outputs": [],
   "source": [
    "### Download Training Data and Test Features ###\n",
    "train = pd.read_csv('TTT_train.csv')\n",
    "test = pd.read_csv('TTT_test_features.csv',index_col = 'ID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's See What We Are Up Against ###\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T19:52:17.042105Z",
     "start_time": "2019-03-19T19:52:17.031241Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([9, 0, 4, 6, 8, 5, 1, 2, 7, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "9    363\n",
       "0    314\n",
       "8    168\n",
       "7     81\n",
       "6     69\n",
       "5     66\n",
       "4     65\n",
       "3     46\n",
       "2     36\n",
       "1     36\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train['label'].nunique()\n",
    "train['label'].unique()\n",
    "train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T20:18:44.003580Z",
     "start_time": "2019-03-19T20:18:43.827093Z"
    }
   },
   "outputs": [],
   "source": [
    "features_all = [\"f\"+str(i) for i in range(1256)]\n",
    "X = train[features_all]\n",
    "y = train['label']\n",
    "y_ohe = train['label'].values.reshape(-1,1)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "target_encoder = OneHotEncoder(sparse=False)\n",
    "y_ohe = target_encoder.fit_transform(y_ohe)\n",
    "y_ohe = pd.DataFrame(y_ohe)\n",
    "\n",
    "X_test = test[features_all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T04:43:46.519413Z",
     "start_time": "2019-03-21T04:43:46.507217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_score_weighted': 0.13182779519665772,\n",
       " 'f1_score_macro': 0.04517734909769758,\n",
       " 'accuracy': 0.29180064308681675}"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.full(y.shape,9)\n",
    "score(y,y_ohe,None,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T05:13:15.721582Z",
     "start_time": "2019-03-21T05:13:15.706146Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_score_weighted': 0.17819448194354215,\n",
       " 'f1_score_macro': 0.10576762759706675,\n",
       " 'accuracy': 0.1760450160771704}"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.random.choice(np.arange(0, 10), p=(y.value_counts()/y.shape).sort_index().values,size=y.shape)\n",
    "score(y,y_ohe,None,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T05:18:57.399308Z",
     "start_time": "2019-03-21T05:18:57.394488Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_score_better_than_random(score):\n",
    "    if score[\"accuracy\"]>0.32:\n",
    "        return True\n",
    "    if score['f1_score_weighted']>0.18:\n",
    "        return True\n",
    "    if score['f1_score_macro']>0.11:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Target Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T05:19:24.768749Z",
     "start_time": "2019-03-21T05:19:24.750074Z"
    }
   },
   "outputs": [],
   "source": [
    "def score(y_true,y_ohe,y_score,y_pred):\n",
    "    from sklearn.metrics import average_precision_score\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    \n",
    "    asc = accuracy_score(y_true, y_pred)\n",
    "    f1_score_weighted = f1_score(y_true, y_pred,average='weighted')\n",
    "    f1_score_macro = f1_score(y_true, y_pred,average='macro')\n",
    "    result = {\"f1_score_weighted\":f1_score_weighted,\"f1_score_macro\":f1_score_macro,\"accuracy\":asc}\n",
    "    # print(result)\n",
    "    \n",
    "    return result\n",
    "            \n",
    "\n",
    "def process_cumulative_res(cv_results):\n",
    "    cr = {}\n",
    "    res_len = len(cv_results)\n",
    "    for key,res in cv_results.items():\n",
    "        for k,v in res.items():\n",
    "            cr[k] = cr[k]+v if k in cr else v\n",
    "            # cr[k+\"_std\"] = cr[k+\"_std\"]+[v] if k+\"_std\" in cr else [v]\n",
    "    for k,v in cr.items():\n",
    "        if k.endswith(\"std\"):\n",
    "            pass\n",
    "            # cr[k] = np.std(cr[k])\n",
    "        if not k.endswith(\"std\"):\n",
    "            cr[k] = cr[k]/res_len\n",
    "    return cr\n",
    "    \n",
    "def summarize_res(train_results,test_results):\n",
    "    \n",
    "    train_results = process_cumulative_res(train_results)\n",
    "    test_results = process_cumulative_res(test_results)\n",
    "    \n",
    "    columns = [\"train\",\"test\"]\n",
    "    keys = train_results.keys()\n",
    "    values = [[train_results[key],test_results[key]] for key in keys]\n",
    "    res = pd.DataFrame(values,columns=columns,index=keys)\n",
    "    res['diff'] = res['train'] - res['test']\n",
    "    avg_test_scores = dict(res['test'])\n",
    "    \n",
    "    return res,is_score_better_than_random(avg_test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T20:18:31.325352Z",
     "start_time": "2019-03-19T20:18:31.314828Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "def cross_validate_classifier(build_model,X,y,y_ohe,scoring_fn,cv=10,use=10):\n",
    "    X, y = shuffle(X, y)\n",
    "    kf = KFold(n_splits=cv)\n",
    "    results_train = {}\n",
    "    results = {}\n",
    "    i = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train,y_train,y_ohe_train = X.iloc[train_index],y.iloc[train_index],y_ohe.iloc[train_index]\n",
    "        X_test, y_test,y_ohe_test = X.iloc[test_index], y.iloc[test_index],y_ohe.iloc[test_index]\n",
    "        model = build_model()\n",
    "        start = time.time()\n",
    "        model.fit(X_train,y_train)\n",
    "        y_score = model.predict_proba(X_test)\n",
    "        y_pred = np.argmax(y_score, axis=1)\n",
    "        # best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "        res = scoring_fn(y_test,y_ohe_test, y_score, y_pred)\n",
    "        \n",
    "        y_score = model.predict_proba(X_train)\n",
    "        y_pred = np.argmax(y_score, axis=1)\n",
    "        res_train = scoring_fn(y_train,y_ohe_train, y_score, y_pred)\n",
    "        results[i] = res\n",
    "        results_train[i] = res_train\n",
    "        end = time.time()\n",
    "        i = i+1\n",
    "        if i>=use:\n",
    "            break\n",
    "    gc.collect()\n",
    "    return summarize_res(results_train,results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T05:19:49.522363Z",
     "start_time": "2019-03-21T05:19:33.645894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                      train      test      diff\n",
       " f1_score_weighted  0.823480  0.711610  0.111870\n",
       " f1_score_macro     0.819093  0.662466  0.156627\n",
       " accuracy           0.830386  0.728265  0.102122, True)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=10,learning_rate=0.1,gamma=0,\n",
    "                               missing=np.NaN,max_depth=3,n_jobs=multiprocessing.cpu_count(),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "cross_validate_classifier(build_model,X,y,y_ohe,score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=4,learning_rate=0.5,gamma=1,\n",
    "                               missing=np.NaN,max_depth=5,nthread=multiprocessing.cpu_count(),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "    \n",
    "cross_validate_classifier(build_model,X,y,y_ohe,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=4,learning_rate=0.8,gamma=1,\n",
    "                               missing=np.NaN,max_depth=5,nthread=multiprocessing.cpu_count(),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "    \n",
    "cross_validate_classifier(build_model,X,y,y_ohe,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=5,learning_rate=0.5,gamma=1,\n",
    "                               missing=np.NaN,max_depth=5,nthread=multiprocessing.cpu_count(),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "    \n",
    "cross_validate_classifier(build_model,X,y,y_ohe,score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Minimise the diff while giving highest test scores\n",
    "- Try AutoEncoder to Reduce Dims\n",
    "- Between which classes model gets confused"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Submission Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=5,learning_rate=0.5,gamma=1,\n",
    "                               missing=np.NaN,max_depth=5,nthread=multiprocessing.cpu_count(),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "\n",
    "model = build_model()\n",
    "model.fit(X,y)\n",
    "y_score = model.predict_proba(X)\n",
    "y_pred = np.argmax(y_score, axis=1)\n",
    "score(y,y_ohe,y_score,y_pred)\n",
    "\n",
    "y_pred[0:20]\n",
    "train['label'].head(20).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_importance(model,features):\n",
    "    \"\"\"\n",
    "\n",
    "    :param model: Model object which has `feature_importances_`\n",
    "    :param features: features/columns that were given to model, these must be in same order as given to model\n",
    "    :return: DataFrame with sorted feature importances\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        fi=model.feature_importances_\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        fi = model.coef_\n",
    "    else:\n",
    "        raise AttributeError('No attribute: feature_importances_ or  coef_')\n",
    "    df_i=pd.DataFrame({\"feature\":features,\"importance\":fi})\n",
    "    df_i[\"importance\"] = df_i[\"importance\"]*100\n",
    "    return df_i.sort_values(\"importance\",ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = model.predict_proba(X_test)\n",
    "y_pred = np.argmax(y_score, axis=1)\n",
    "y_pred[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Using fi from previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi = feature_importance(model,X.columns)\n",
    "features_no_imp_1 = df_fi[df_fi['importance']==0]['feature'].values\n",
    "len(features_no_imp_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_imp_1 = list(set(features_all) - set(features_no_imp_1))\n",
    "len(features_imp_1)\n",
    "X = train[features_imp_1]\n",
    "X_test = test[features_imp_1]\n",
    "\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=5,learning_rate=0.5,gamma=1,\n",
    "                               missing=np.NaN,max_depth=5,nthread=multiprocessing.cpu_count(),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "    \n",
    "cross_validate_classifier(build_model,X,y,y_ohe,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = train[features_no_imp_1]\n",
    "X_test = test[features_no_imp_1]\n",
    "\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=5,learning_rate=0.5,gamma=1,\n",
    "                               missing=np.NaN,max_depth=5,nthread=multiprocessing.cpu_count(),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "    \n",
    "cross_validate_classifier(build_model,X,y,y_ohe,score)\n",
    "model = build_model()\n",
    "model.fit(X,y)\n",
    "df_fi = feature_importance(model,X.columns)\n",
    "features_no_imp_2 = df_fi[df_fi['importance']==0]['feature'].values\n",
    "len(features_no_imp_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_imp_2 = list(set(features_no_imp_1) - set(features_no_imp_2))\n",
    "X = train[features_imp_2]\n",
    "X_test = test[features_imp_2]\n",
    "\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=5,learning_rate=0.5,gamma=1,\n",
    "                               missing=np.NaN,max_depth=5,nthread=multiprocessing.cpu_count(),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "    \n",
    "cross_validate_classifier(build_model,X,y,y_ohe,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[features_no_imp_2]\n",
    "X_test = test[features_no_imp_2]\n",
    "\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=5,learning_rate=0.5,gamma=1,\n",
    "                               missing=np.NaN,max_depth=5,nthread=multiprocessing.cpu_count(),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "    \n",
    "cross_validate_classifier(build_model,X,y,y_ohe,score)\n",
    "model = build_model()\n",
    "model.fit(X,y)\n",
    "df_fi = feature_importance(model,X.columns)\n",
    "features_no_imp_3 = df_fi[df_fi['importance']==0]['feature'].values\n",
    "len(features_no_imp_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_imp_3 = list(set(features_no_imp_2) - set(features_no_imp_3))\n",
    "X = train[features_imp_3]\n",
    "X_test = test[features_imp_3]\n",
    "\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=5,learning_rate=0.5,gamma=1,\n",
    "                               missing=np.NaN,max_depth=5,nthread=multiprocessing.cpu_count(),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "    \n",
    "cross_validate_classifier(build_model,X,y,y_ohe,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[features_no_imp_3]\n",
    "X_test = test[features_no_imp_3]\n",
    "\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=5,learning_rate=0.5,gamma=1,\n",
    "                               missing=np.NaN,max_depth=5,nthread=multiprocessing.cpu_count(),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "    \n",
    "cross_validate_classifier(build_model,X,y,y_ohe,score)\n",
    "model = build_model()\n",
    "model.fit(X,y)\n",
    "df_fi = feature_importance(model,X.columns)\n",
    "features_no_imp_4 = df_fi[df_fi['importance']==0]['feature'].values\n",
    "len(features_no_imp_4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_imp_4 = list(set(features_no_imp_3) - set(features_no_imp_4))\n",
    "X = train[features_imp_4]\n",
    "X_test = test[features_imp_4]\n",
    "\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=5,learning_rate=0.5,gamma=1,\n",
    "                               missing=np.NaN,max_depth=5,nthread=multiprocessing.cpu_count(),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "    \n",
    "cross_validate_classifier(build_model,X,y,y_ohe,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[features_no_imp_4]\n",
    "X_test = test[features_no_imp_4]\n",
    "\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=5,learning_rate=0.5,gamma=1,\n",
    "                               missing=np.NaN,max_depth=5,nthread=multiprocessing.cpu_count(),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "    \n",
    "cross_validate_classifier(build_model,X,y,y_ohe,score)\n",
    "model = build_model()\n",
    "model.fit(X,y)\n",
    "df_fi = feature_importance(model,X.columns)\n",
    "features_no_imp_5 = df_fi[df_fi['importance']==0]['feature'].values\n",
    "len(features_no_imp_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_imp_5 = list(set(features_no_imp_4) - set(features_no_imp_5))\n",
    "X = train[features_imp_5]\n",
    "X_test = test[features_imp_5]\n",
    "\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=5,learning_rate=0.5,gamma=1,\n",
    "                               missing=np.NaN,max_depth=5,nthread=multiprocessing.cpu_count(),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "    \n",
    "cross_validate_classifier(build_model,X,y,y_ohe,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features_imp_1) + len(features_imp_2) + len(features_imp_3) + len(features_imp_4) + len(features_imp_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(features_imp_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = list(set(features_imp_1+features_imp_2+features_imp_3+features_imp_4+features_imp_5))\n",
    "len(new_features)\n",
    "X = train[new_features]\n",
    "X_test = test[new_features]\n",
    "\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=5,learning_rate=0.5,gamma=1,\n",
    "                               missing=np.NaN,max_depth=5,nthread=multiprocessing.cpu_count(),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "    \n",
    "cross_validate_classifier(build_model,X,y,y_ohe,score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model based on feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "def cross_validate_classifier_voted(build_model,X,y,y_ohe,feature_lists,weights,scoring_fn,cv=10,use=10):\n",
    "    X, y = shuffle(X, y)\n",
    "    kf = KFold(n_splits=cv)\n",
    "    results_train = {}\n",
    "    results = {}\n",
    "    i = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train,y_train,y_ohe_train = X.iloc[train_index],y.iloc[train_index],y_ohe.iloc[train_index]\n",
    "        X_test, y_test,y_ohe_test = X.iloc[test_index], y.iloc[test_index],y_ohe.iloc[test_index]\n",
    "        \n",
    "        start = time.time()\n",
    "        models = []\n",
    "        for i,features in enumerate(feature_lists):\n",
    "            model = build_model()\n",
    "            assert len(set(X_train.columns).intersection(set(features)))==len(features)\n",
    "            model.fit(X_train[features],y_train)\n",
    "            models.append(model)\n",
    "        \n",
    "        y_scores = []\n",
    "        for i,model in enumerate(models):\n",
    "            weight = weights[i]\n",
    "            features = feature_lists[i]\n",
    "            y_score = model.predict_proba(X_test[features])\n",
    "            y_scores.append(y_score)\n",
    "        y_score = np.average(np.array(y_scores),axis=0,weights=weights) \n",
    "        y_pred = np.argmax(y_score, axis=1)\n",
    "        res = scoring_fn(y_test,y_ohe_test, y_score, y_pred)\n",
    "        \n",
    "        y_scores = []\n",
    "        for i,model in enumerate(models):\n",
    "            weight = weights[i]\n",
    "            features = feature_lists[i]\n",
    "            y_score = model.predict_proba(X_train[features])\n",
    "            y_scores.append(y_score)\n",
    "        y_score = np.average(np.array(y_scores),axis=0,weights=weights) \n",
    "        y_pred = np.argmax(y_score, axis=1)\n",
    "        res_train = scoring_fn(y_train,y_ohe_train, y_score, y_pred)\n",
    "        results[i] = res\n",
    "        results_train[i] = res_train\n",
    "        end = time.time()\n",
    "        i = i+1\n",
    "        if i>=use:\n",
    "            break\n",
    "    gc.collect()\n",
    "    return summarize_res(results_train,results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "X = train[features_all]\n",
    "X_test = test[features_all]\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=5,learning_rate=0.5,gamma=1,\n",
    "                               missing=np.NaN,max_depth=5,nthread=multiprocessing.cpu_count(),objective='multi:softmax')\n",
    "    return classifier2\n",
    "    \n",
    "cross_validate_classifier_voted(build_model,X,y,y_ohe,[features_imp_1,features_imp_2,features_imp_3,features_imp_4,features_imp_5],[0.4,0.35,0.12,0.08,0.05],score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0) # only difference\n",
    "\n",
    "\n",
    "\n",
    "weights = softmax(np.array([77,75,36,24,14]))\n",
    "cross_validate_classifier_voted(build_model,X,y,y_ohe,[features_imp_1,features_imp_2,features_imp_3,features_imp_4,features_imp_5],weights,score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[features_all]\n",
    "X_test = test[features_all]\n",
    "weights = [0.45,0.35,0.2]\n",
    "cross_validate_classifier_voted(build_model,X,y,y_ohe,[features_imp_1,features_imp_2,features_imp_3],weights,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[features_all]\n",
    "X_test = test[features_all]\n",
    "weights = softmax(np.array([77,75,36]))\n",
    "cross_validate_classifier_voted(build_model,X,y,y_ohe,[features_imp_1,features_imp_2,features_imp_3],weights,score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[features_all]\n",
    "X_test = test[features_all]\n",
    "weights = softmax(np.array([70,69,36,35]))\n",
    "cross_validate_classifier_voted(build_model,X,y,y_ohe,[features_imp_1,features_imp_2,features_imp_3,features_imp_4+features_imp_5],weights,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features_imp_1)\n",
    "len(features_imp_2)\n",
    "len(features_imp_3)\n",
    "len(features_imp_4)\n",
    "len(features_imp_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[features_all]\n",
    "X_test = test[features_all]\n",
    "feature_lists = [features_imp_1,features_imp_2,features_imp_3,features_imp_4,features_imp_5]\n",
    "weights = softmax(np.array([77,75,36,24,14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[features_all]\n",
    "X_test = test[features_all]\n",
    "feature_lists = [features_imp_1,features_imp_2,features_imp_3]\n",
    "weights = softmax(np.array([77,75,36]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_lists = [features_imp_1,features_imp_2,features_imp_3,features_imp_4+features_imp_5]\n",
    "weights = softmax(np.array([70,69,36,35]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "models = []\n",
    "for i,features in enumerate(feature_lists):\n",
    "    model = build_model()\n",
    "    assert len(set(X.columns).intersection(set(features)))==len(features)\n",
    "    model.fit(X[features],y)\n",
    "    models.append(model)\n",
    "    \n",
    "y_scores = []\n",
    "for i,model in enumerate(models):\n",
    "    features = feature_lists[i]\n",
    "    y_score = model.predict_proba(X[features])\n",
    "    y_scores.append(y_score)\n",
    "y_score = np.average(np.array(y_scores),axis=0,weights=weights) \n",
    "y_pred = np.argmax(y_score, axis=1)\n",
    "score(y,y_ohe, y_score, y_pred)\n",
    "\n",
    "y_scores = []\n",
    "for i,model in enumerate(models):\n",
    "    weight = weights[i]\n",
    "    features = feature_lists[i]\n",
    "    y_score = model.predict_proba(X_test[features])\n",
    "    y_scores.append(y_score)\n",
    "y_score = np.average(np.array(y_scores),axis=0,weights=weights) \n",
    "y_pred = np.argmax(y_score, axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Iterative XGBoost on last step unimportant features = F_sets_1 (F_set_1,F_set_2,...)\n",
    "- Remaining Features used 1 by 1 with models+F_sets_1 to find useful features.\n",
    "- Use the further remaining features 1 by 1 to see if any gives useful information.\n",
    "- Eliminate Covariate shifts from these.\n",
    "- Use AutoEncoder to these features to compress them. AutoEncoders can be used on test set as well.\n",
    "- Resample classes with low number of Data points\n",
    "- Use XGBoost and DNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T20:43:16.469292Z",
     "start_time": "2019-03-19T20:43:16.465983Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T20:45:57.678421Z",
     "start_time": "2019-03-19T20:45:57.670738Z"
    }
   },
   "outputs": [],
   "source": [
    "def iterative_feature_detection(X,y,initial_features,build_model,num_iters=10):\n",
    "    features_with_imp = []\n",
    "    features_current = list(set(initial_features))\n",
    "    df_fis = []\n",
    "    # repeat\n",
    "    for i in range(num_iters):\n",
    "        if len(features_current)<=1:\n",
    "            break\n",
    "        X_train = X[features_current]\n",
    "        model = build_model()\n",
    "        model.fit(X_train,y)\n",
    "        df_fi = model_utils.feature_importance(model,X_train.columns)\n",
    "        if df_fi.shape[0]<=0:\n",
    "            break\n",
    "        features_no_imp_current = df_fi[(df_fi['importance']==0)|(np.isnan(df_fi['importance']))]['feature'].values\n",
    "        \n",
    "        features_imp_current = list(set(features_current) - set(features_no_imp_current))\n",
    "        features_with_imp.append(features_imp_current)\n",
    "        features_current = list(set(features_no_imp_current))\n",
    "        print(len(features_imp_current))\n",
    "        if len(features_imp_current)<1:\n",
    "            break\n",
    "        df_fis.append(df_fi)\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    return features_with_imp,df_fis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T20:47:00.871463Z",
     "start_time": "2019-03-19T20:46:36.078323Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "75\n",
      "36\n",
      "24\n",
      "17\n",
      "12\n",
      "1\n",
      "0\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "85\n",
      "36\n",
      "15\n",
      "9\n",
      "8\n",
      "1\n",
      "0\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n",
      "72\n",
      "40\n",
      "21\n",
      "15\n",
      "10\n",
      "4\n",
      "1\n",
      "0\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=5,learning_rate=0.5,gamma=1,\n",
    "                               missing=np.NaN,max_depth=5,n_jobs=int(multiprocessing.cpu_count()/2),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "\n",
    "features_with_imp_1,df_fis = iterative_feature_detection(X,y,features_all,build_model)\n",
    "len(list(more_itertools.flatten(features_with_imp_1)))\n",
    "\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=10,learning_rate=0.4,gamma=1,\n",
    "                               missing=np.NaN,max_depth=4,n_jobs=int(multiprocessing.cpu_count()/2),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "\n",
    "features_with_imp_2,df_fis = iterative_feature_detection(X,y,features_all,build_model)\n",
    "len(list(more_itertools.flatten(features_with_imp_2)))\n",
    "\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=4,learning_rate=0.5,gamma=1,\n",
    "                               missing=np.NaN,max_depth=6,n_jobs=int(multiprocessing.cpu_count()/2),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "\n",
    "features_with_imp_3,df_fis = iterative_feature_detection(X,y,features_all,build_model)\n",
    "len(list(more_itertools.flatten(features_with_imp_3)))\n",
    "\n",
    "\n",
    "len(set(list(more_itertools.flatten(features_with_imp_1))+list(more_itertools.flatten(features_with_imp_2))+list(more_itertools.flatten(features_with_imp_3))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T20:49:11.088160Z",
     "start_time": "2019-03-19T20:49:11.084614Z"
    }
   },
   "outputs": [],
   "source": [
    "features_with_imp = features_with_imp_1+features_with_imp_2+features_with_imp_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T21:50:37.782740Z",
     "start_time": "2019-03-19T21:50:37.775545Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tnrange, tqdm_notebook as tqdm\n",
    "def interactive_feature_detection(X,y,features_with_imp,features_all,build_model):\n",
    "    features_with_no_imp = set(features_all) - set(list(more_itertools.flatten(features_with_imp)))\n",
    "    features_interactive_imp = []\n",
    "    \n",
    "    for feature in tqdm(features_with_no_imp):\n",
    "        for imp_features in features_with_imp:\n",
    "            gc.collect()\n",
    "            new_trail_set = imp_features+[feature]\n",
    "            X_train = X[new_trail_set]\n",
    "            model = build_model()\n",
    "            model.fit(X_train,y)\n",
    "            df_fi = model_utils.feature_importance(model,X_train.columns)\n",
    "            if df_fi.shape[0]<=0:\n",
    "                break\n",
    "            imp = df_fi[df_fi['feature']==feature]['importance'].values[0]\n",
    "            if imp>0:\n",
    "                print(feature,imp)\n",
    "                features_interactive_imp.append(feature)\n",
    "                break\n",
    "    return features_interactive_imp\n",
    "\n",
    "                \n",
    "\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T00:34:33.369328Z",
     "start_time": "2019-03-19T21:50:38.760474Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cbd39cf522c46e0964a9b5c23e6998d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1012), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found feature =  f712 1.3894463\n",
      "Found feature =  f327 5.548524\n",
      "Found feature =  f451 5.485139\n",
      "Found feature =  f568 1.3476895\n",
      "Found feature =  f1183 5.548524\n",
      "Found feature =  f279 5.548524\n",
      "Found feature =  f570 5.548524\n",
      "Found feature =  f869 1.1088117\n",
      "Found feature =  f305 5.548524\n",
      "Found feature =  f487 5.548524\n",
      "Found feature =  f493 5.548524\n",
      "Found feature =  f479 5.548524\n",
      "Found feature =  f672 5.548524\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=5,learning_rate=0.5,gamma=1,\n",
    "                               missing=np.NaN,max_depth=5,n_jobs=int(multiprocessing.cpu_count()/2),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "features_interactive_imp_1 = interactive_feature_detection(X,y,features_with_imp,features_all,build_model)\n",
    "len(features_interactive_imp_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T06:02:57.524488Z",
     "start_time": "2019-03-20T06:02:56.894031Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1_score_weighted</th>\n",
       "      <td>0.132414</td>\n",
       "      <td>0.133002</td>\n",
       "      <td>-0.000589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score_macro</th>\n",
       "      <td>0.045398</td>\n",
       "      <td>0.045308</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.292069</td>\n",
       "      <td>0.290213</td>\n",
       "      <td>0.001856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      train      test      diff\n",
       "f1_score_weighted  0.132414  0.133002 -0.000589\n",
       "f1_score_macro     0.045398  0.045308  0.000089\n",
       "accuracy           0.292069  0.290213  0.001856"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate_classifier(build_model,X[features_interactive_imp_1],y,y_ohe,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T09:43:55.796794Z",
     "start_time": "2019-03-20T06:07:49.820223Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e6fe6edd01474c80a4cfec001edab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1012), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found feature =  f625 31.712914\n",
      "Found feature =  f1016 34.141712\n",
      "Found feature =  f70 31.712914\n",
      "Found feature =  f732 29.845074\n",
      "Found feature =  f1076 31.712914\n",
      "Found feature =  f327 34.141712\n",
      "Found feature =  f618 31.712914\n",
      "Found feature =  f112 31.712914\n",
      "Found feature =  f451 1.8503435\n",
      "Found feature =  f1050 31.712914\n",
      "Found feature =  f944 34.141712\n",
      "Found feature =  f550 31.712914\n",
      "Found feature =  f985 0.6301911\n",
      "Found feature =  f490 34.141712\n",
      "Found feature =  f176 34.141712\n",
      "Found feature =  f603 31.712914\n",
      "Found feature =  f1183 34.141712\n",
      "Found feature =  f279 34.141712\n",
      "Found feature =  f83 34.141712\n",
      "Found feature =  f1142 31.712914\n",
      "Found feature =  f869 29.933977\n",
      "Found feature =  f407 34.141712\n",
      "Found feature =  f910 34.141712\n",
      "Found feature =  f462 34.141712\n",
      "Found feature =  f1113 31.712914\n",
      "Found feature =  f228 31.712914\n",
      "Found feature =  f480 31.712914\n",
      "Found feature =  f292 31.712914\n",
      "Found feature =  f345 34.141712\n",
      "Found feature =  f342 31.712914\n",
      "Found feature =  f319 34.141712\n",
      "Found feature =  f352 34.141712\n",
      "Found feature =  f35 31.712914\n",
      "Found feature =  f1244 34.141712\n",
      "Found feature =  f977 32.190853\n",
      "Found feature =  f479 34.141712\n",
      "Found feature =  f466 31.712914\n",
      "Found feature =  f260 34.141712\n",
      "Found feature =  f265 31.712914\n",
      "Found feature =  f121 31.712914\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=20,learning_rate=0.3,gamma=1,\n",
    "                               missing=np.NaN,max_depth=3,n_jobs=int(multiprocessing.cpu_count()/2),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "features_interactive_imp_2 = interactive_feature_detection(X,y,features_with_imp,features_all,build_model)\n",
    "len(features_interactive_imp_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T14:01:36.071319Z",
     "start_time": "2019-03-20T14:01:34.571681Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1_score_weighted</th>\n",
       "      <td>0.132899</td>\n",
       "      <td>0.131879</td>\n",
       "      <td>0.001019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score_macro</th>\n",
       "      <td>0.045598</td>\n",
       "      <td>0.044953</td>\n",
       "      <td>0.000645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.292158</td>\n",
       "      <td>0.290213</td>\n",
       "      <td>0.001945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      train      test      diff\n",
       "f1_score_weighted  0.132899  0.131879  0.001019\n",
       "f1_score_macro     0.045598  0.044953  0.000645\n",
       "accuracy           0.292158  0.290213  0.001945"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate_classifier(build_model,X[features_interactive_imp_2],y,y_ohe,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_with_imp.append(features_interactive_imp_1)\n",
    "features_with_imp.append(features_interactive_imp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features_with_imp)\n",
    "features_with_imp = list(filter(lambda x:len(x)>0,features_with_imp))\n",
    "len(features_with_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T04:39:38.991909Z",
     "start_time": "2019-03-20T20:38:58.837435Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d698006ee95e4c2b8e3f4ab63be61dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=965), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found feature =  f826 53.306847\n",
      "Found feature =  f752 0.89390993\n",
      "Found feature =  f113 53.306847\n",
      "Found feature =  f861 25.379416\n",
      "Found feature =  f217 2.3803673\n",
      "Found feature =  f90 0.89390993\n",
      "Found feature =  f124 1.2240816\n",
      "Found feature =  f12 25.379416\n",
      "Found feature =  f396 53.306847\n",
      "Found feature =  f418 3.1616702\n",
      "Found feature =  f106 53.306847\n",
      "Found feature =  f747 0.06684949\n",
      "Found feature =  f460 53.306847\n",
      "Found feature =  f477 0.07404636\n",
      "Found feature =  f172 53.306847\n",
      "Found feature =  f1198 1.4395225\n",
      "Found feature =  f3 1.5556809\n",
      "Found feature =  f912 1.1477809\n",
      "Found feature =  f496 38.07\n",
      "Found feature =  f1229 53.306847\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-251-1d5b047a198a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclassifier2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfeatures_interactive_imp_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minteractive_feature_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures_with_imp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures_all\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_interactive_imp_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-ab04f10678d4>\u001b[0m in \u001b[0;36minteractive_feature_detection\u001b[0;34m(X, y, features_with_imp, features_all, build_model)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_with_no_imp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimp_features\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures_with_imp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mnew_trail_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp_features\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_trail_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=3,learning_rate=0.8,gamma=0,\n",
    "                               missing=np.NaN,max_depth=6,n_jobs=int(multiprocessing.cpu_count()/2),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "features_interactive_imp_3 = interactive_feature_detection(X,y,features_with_imp,features_all,build_model)\n",
    "len(features_interactive_imp_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T04:39:38.999929Z",
     "start_time": "2019-03-20T20:36:00.292Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "features_with_imp.append(features_interactive_imp_3)\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=18,learning_rate=0.3,gamma=0,\n",
    "                               missing=np.NaN,max_depth=3,n_jobs=int(multiprocessing.cpu_count()/2),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "features_interactive_imp_4 = interactive_feature_detection(X,y,features_with_imp,features_all,build_model)\n",
    "len(features_interactive_imp_4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T04:39:39.001567Z",
     "start_time": "2019-03-20T20:36:02.764Z"
    }
   },
   "outputs": [],
   "source": [
    "features_with_imp.append(features_interactive_imp_4)\n",
    "def build_model():\n",
    "    classifier = RUSBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=4),n_estimators=10, \n",
    "                                    learning_rate=0.4, algorithm='SAMME.R', \n",
    "                                    sampling_strategy='auto', replacement=False)\n",
    "    return classifier    \n",
    "    return classifier2\n",
    "features_interactive_imp_5 = interactive_feature_detection(X,y,features_with_imp,features_all,build_model)\n",
    "len(features_interactive_imp_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T04:39:39.003192Z",
     "start_time": "2019-03-20T20:36:06.880Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "features_with_imp.append(features_interactive_imp_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T04:39:39.006710Z",
     "start_time": "2019-03-20T20:36:07.322Z"
    }
   },
   "outputs": [],
   "source": [
    "features_useful = list(set(more_itertools.flatten(features_with_imp)))\n",
    "\n",
    "len(features_useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:17:17.699472Z",
     "start_time": "2019-03-20T20:16:35.159Z"
    }
   },
   "outputs": [],
   "source": [
    "# Covariate shift\n",
    "# Correlated features with target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use remaining features in groups of 3 in a DNN , if accuracy > 25, then check feature imp through xgb and take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample\n",
    "# auto-encoder components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AutoEncoderKerasDNN:\n",
    "    def __init__(self,n_components=16,lr=0.01,\n",
    "                 n_iter=[150,100,75], columns=[], prefix=\"autoenc_\",\n",
    "                 store_train_data=False,\n",
    "                 store_transform_data=False,\n",
    "                 scale_input=False, raise_null=True,inplace=True,verbose=True,):\n",
    "        self.n_components = n_components\n",
    "        self.prefix = prefix\n",
    "        self.cols = columns\n",
    "        assert len(columns) > 0 or prefixes is not None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        self.imp_inf = SimpleImputer(missing_values=np.inf, strategy='mean')\n",
    "        self.train = None\n",
    "        self.store_train_data = store_train_data\n",
    "        self.store_transform_data = store_transform_data\n",
    "        self.transform_data = None\n",
    "        self.n_iter = n_iter\n",
    "        self.verbose = verbose\n",
    "        self.lr = lr\n",
    "        self.inplace = inplace\n",
    "        self.scale_input=scale_input\n",
    "\n",
    "    def check_null_(self, X):\n",
    "        nans = np.isnan(X)\n",
    "        infs = np.isinf(X)\n",
    "        nan_summary = np.sum(np.logical_or(nans, infs))\n",
    "        if nan_summary > 0:\n",
    "            raise ValueError(\"nans/inf in frame = %s\" % (nan_summary))\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        if self.store_train_data:\n",
    "            self.train = (X.copy(),y.copy(),sample_weight)\n",
    "        \n",
    "        cols = self.cols\n",
    "        X = X[cols]\n",
    "        \n",
    "        X = self.imp.fit_transform(X)\n",
    "        X = self.imp_inf.fit_transform(X)\n",
    "        \n",
    "        if self.scale_input:\n",
    "            X = self.scaler.fit_transform(X)\n",
    "        self.check_null_(X)\n",
    "        \n",
    "        X1,X2,X3 = np.split(X, [int(.33*len(X)), int(.66*len(X))])\n",
    "        \n",
    "        X1,X2,X3 = pd.DataFrame(X1),pd.DataFrame(X2),pd.DataFrame(X3)\n",
    "        \n",
    "        input_layer = Input(shape=(X.shape[1],))\n",
    "        encoded = Dense(self.n_components * 4, activation='elu')(input_layer)\n",
    "        encoded = Dense(self.n_components * 2, activation='elu')(encoded)\n",
    "        bottleneck = Dense(self.n_components, activation='elu')(encoded)\n",
    "\n",
    "        decoded = Dense(self.n_components * 2, activation='elu')(bottleneck)\n",
    "        decoded = Dense(self.n_components * 4, activation='elu')(decoded)\n",
    "        decoded = Dense(X.shape[1], activation='elu')(decoded)\n",
    "\n",
    "        \n",
    "        autoencoder = Model(input_layer, decoded)\n",
    "        encoder = Model(input_layer, bottleneck)\n",
    "        \n",
    "        adam = optimizers.Adam(lr=self.lr, clipnorm=0.75, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False)\n",
    "        autoencoder.compile(optimizer=adam, loss=\"mean_squared_error\")\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00005,epsilon=0.0001)\n",
    "        reduce_lr2 = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.00005,epsilon=0.0001)\n",
    "        terminate_on_nan = TerminateOnNaN()\n",
    "        es = EarlyStopping(monitor='val_loss', min_delta=0.000001, patience=8, verbose=0,)\n",
    "        es2 = EarlyStopping(monitor='val_loss', min_delta=0.000001, patience=6, verbose=0,)\n",
    "        # print(K.get_value(model.optimizer.lr))\n",
    "        \n",
    "        X_train = pd.concat((X1,X2,X3),axis=0)\n",
    "        autoencoder.fit(X_train, X_train,\n",
    "                        epochs=2,\n",
    "                        batch_size=1024,\n",
    "                        shuffle=True,\n",
    "                        verbose=self.verbose,\n",
    "                        callbacks=[es,terminate_on_nan,reduce_lr])\n",
    "        \n",
    "        X_train,X_val = pd.concat((X1,X2),axis=0),X3\n",
    "        autoencoder.fit(X_train, X_train,\n",
    "                        epochs=self.n_iter[0],\n",
    "                        batch_size=4096,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(X_val, X_val),\n",
    "                        verbose=self.verbose,\n",
    "                        callbacks=[es,terminate_on_nan,reduce_lr])\n",
    "        \n",
    "        print(\"LR: \",K.get_value(autoencoder.optimizer.lr))\n",
    "        K.set_value(autoencoder.optimizer.lr, self.lr/2)\n",
    "        \n",
    "        X_train,X_val = pd.concat((X2,X3),axis=0),X1\n",
    "        autoencoder.fit(X_train, X_train,\n",
    "                        epochs=self.n_iter[1],\n",
    "                        batch_size=4096,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(X_val, X_val),\n",
    "                        verbose=self.verbose,\n",
    "                        callbacks=[es,terminate_on_nan,reduce_lr2])\n",
    "        \n",
    "        \n",
    "        K.set_value(autoencoder.optimizer.lr, self.lr/2)\n",
    "        X_train = pd.concat((X1,X2,X3),axis=0)\n",
    "        autoencoder.fit(X_train, X_train,\n",
    "                        epochs=2,\n",
    "                        batch_size=1024,\n",
    "                        shuffle=True,\n",
    "                        verbose=self.verbose,\n",
    "                        callbacks=[es,terminate_on_nan,reduce_lr])\n",
    "        \n",
    "        \n",
    "        print(\"LR: \",K.get_value(autoencoder.optimizer.lr))\n",
    "        K.set_value(autoencoder.optimizer.lr, self.lr/2)\n",
    "        print(\"Post value set LR: \",K.get_value(autoencoder.optimizer.lr))\n",
    "        \n",
    "        X_train,X_val = pd.concat((X1,X3),axis=0),X2\n",
    "        autoencoder.fit(X_train, X_train,\n",
    "                        epochs=self.n_iter[2],\n",
    "                        batch_size=4096,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(X_val, X_val),\n",
    "                        verbose=self.verbose,\n",
    "                        callbacks=[es,terminate_on_nan,reduce_lr2])\n",
    "        \n",
    "        K.set_value(autoencoder.optimizer.lr, self.lr/2)\n",
    "        \n",
    "        X_train = pd.concat((X1,X2,X3),axis=0)\n",
    "        autoencoder.fit(X_train, X_train,\n",
    "                        epochs=2,\n",
    "                        batch_size=1024,\n",
    "                        shuffle=True,\n",
    "                        verbose=self.verbose,\n",
    "                        callbacks=[es,terminate_on_nan,reduce_lr])\n",
    "        \n",
    "        \n",
    "        gc.collect()\n",
    "        self.model = encoder\n",
    "        return self\n",
    "\n",
    "    def partial_fit(self, X, y):\n",
    "        return self.fit(X, y)\n",
    "\n",
    "    def transform(self, X, y='ignored'):\n",
    "        if self.store_transform_data:\n",
    "            self.transform_data = (X.copy())\n",
    "        Inp = X\n",
    "        cols = self.cols\n",
    "        Inp = Inp[cols]\n",
    "        Inp = self.imp.transform(Inp)\n",
    "        Inp = self.imp_inf.transform(Inp)\n",
    "        if self.scale_input:\n",
    "            Inp = self.scaler.transform(Inp)\n",
    "        self.check_null_(Inp)\n",
    "        \n",
    "        \n",
    "        results = self.model.predict(Inp)\n",
    "        results = pd.DataFrame(results,columns = list(map(lambda x: self.prefix + str(x), range(0, results.shape[1]))))\n",
    "        results.index = X.index\n",
    "        if not self.inplace:\n",
    "            X = X.copy()\n",
    "        X[results.columns] = results\n",
    "        gc.collect()\n",
    "        return X\n",
    "    \n",
    "\n",
    "    def inverse_transform(self, X, copy=None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def fit_transform(self, X, y=None, sample_weight=None):\n",
    "        self.fit(X, y, sample_weight=sample_weight)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:21:37.149499Z",
     "start_time": "2019-03-20T20:21:37.116574Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class AutoEncoderKerasDNN:\n",
    "    def __init__(self,n_components=16,lr=0.01,\n",
    "                 n_iter=[150,50,25], columns=[], prefix=\"autoenc_\",\n",
    "                 store_train_data=False,\n",
    "                 store_transform_data=False,\n",
    "                 scale_input=False, raise_null=True,inplace=True,verbose=True,):\n",
    "        self.n_components = n_components\n",
    "        self.prefix = prefix\n",
    "        self.cols = columns\n",
    "        assert len(columns) > 0 or prefixes is not None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        self.imp_inf = SimpleImputer(missing_values=np.inf, strategy='mean')\n",
    "        self.train = None\n",
    "        self.store_train_data = store_train_data\n",
    "        self.store_transform_data = store_transform_data\n",
    "        self.transform_data = None\n",
    "        self.n_iter = n_iter\n",
    "        self.verbose = verbose\n",
    "        self.lr = lr\n",
    "        self.inplace = inplace\n",
    "        self.scale_input=scale_input\n",
    "\n",
    "    def check_null_(self, X):\n",
    "        nans = np.isnan(X)\n",
    "        infs = np.isinf(X)\n",
    "        nan_summary = np.sum(np.logical_or(nans, infs))\n",
    "        if nan_summary > 0:\n",
    "            raise ValueError(\"nans/inf in frame = %s\" % (nan_summary))\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        if self.store_train_data:\n",
    "            self.train = (X.copy(),y.copy(),sample_weight)\n",
    "        \n",
    "        cols = self.cols\n",
    "        X = X[cols]\n",
    "        \n",
    "        X = self.imp.fit_transform(X)\n",
    "        X = self.imp_inf.fit_transform(X)\n",
    "        \n",
    "        if self.scale_input:\n",
    "            X = self.scaler.fit_transform(X)\n",
    "        self.check_null_(X)\n",
    "        \n",
    "        X1,X2,X3 = np.split(X, [int(.33*len(X)), int(.66*len(X))])\n",
    "        \n",
    "        X1,X2,X3 = pd.DataFrame(X1),pd.DataFrame(X2),pd.DataFrame(X3)\n",
    "        \n",
    "        input_layer = Input(shape=(X.shape[1],))\n",
    "        encoded = Dense(self.n_components * 4, activation='elu')(input_layer)\n",
    "        encoded = Dense(self.n_components * 2, activation='elu')(encoded)\n",
    "        bottleneck = Dense(self.n_components, activation='elu')(encoded)\n",
    "\n",
    "        decoded = Dense(self.n_components * 2, activation='elu')(bottleneck)\n",
    "        decoded = Dense(self.n_components * 4, activation='elu')(decoded)\n",
    "        decoded = Dense(X.shape[1], activation='elu')(decoded)\n",
    "\n",
    "        \n",
    "        autoencoder = Model(input_layer, decoded)\n",
    "        encoder = Model(input_layer, bottleneck)\n",
    "        \n",
    "        adam = optimizers.Adam(lr=self.lr, clipnorm=0.75, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False)\n",
    "        autoencoder.compile(optimizer=adam, loss=\"mean_squared_error\")\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00005,epsilon=1e-6)\n",
    "        reduce_lr2 = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.00005,epsilon=1e-7)\n",
    "        terminate_on_nan = TerminateOnNaN()\n",
    "        es = EarlyStopping(monitor='val_loss', min_delta=1e-8, patience=8, verbose=0,)\n",
    "        # print(K.get_value(model.optimizer.lr))\n",
    "        \n",
    "        X_train = pd.concat((X1,X2,X3),axis=0)\n",
    "        autoencoder.fit(X_train, X_train,\n",
    "                        epochs=3,\n",
    "                        batch_size=1024,\n",
    "                        shuffle=True,\n",
    "                        verbose=self.verbose,\n",
    "                        callbacks=[es,terminate_on_nan,reduce_lr])\n",
    "        \n",
    "        X_train,X_val = pd.concat((X1,X2),axis=0),X3\n",
    "        autoencoder.fit(X_train, X_train,\n",
    "                        epochs=self.n_iter[0],\n",
    "                        batch_size=4096,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(X_val, X_val),\n",
    "                        verbose=self.verbose,\n",
    "                        callbacks=[es,terminate_on_nan,reduce_lr])\n",
    "        \n",
    "        print(\"LR: \",K.get_value(autoencoder.optimizer.lr))\n",
    "        K.set_value(autoencoder.optimizer.lr, self.lr/2)\n",
    "        \n",
    "        X_train,X_val = pd.concat((X2,X3),axis=0),X1\n",
    "        autoencoder.fit(X_train, X_train,\n",
    "                        epochs=self.n_iter[1],\n",
    "                        batch_size=4096,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(X_val, X_val),\n",
    "                        verbose=self.verbose,\n",
    "                        callbacks=[es,terminate_on_nan,reduce_lr2])\n",
    "        \n",
    "        \n",
    "        K.set_value(autoencoder.optimizer.lr, self.lr)\n",
    "        X_train = pd.concat((X1,X2,X3),axis=0)\n",
    "        autoencoder.fit(X_train, X_train,\n",
    "                        epochs=2,\n",
    "                        batch_size=1024,\n",
    "                        shuffle=True,\n",
    "                        verbose=self.verbose,\n",
    "                        callbacks=[es,terminate_on_nan,reduce_lr2])\n",
    "        \n",
    "        \n",
    "        print(\"LR: \",K.get_value(autoencoder.optimizer.lr))\n",
    "        K.set_value(autoencoder.optimizer.lr, self.lr)\n",
    "        print(\"Post value set LR: \",K.get_value(autoencoder.optimizer.lr))\n",
    "        \n",
    "        X_train,X_val = pd.concat((X1,X3),axis=0),X2\n",
    "        autoencoder.fit(X_train, X_train,\n",
    "                        epochs=self.n_iter[2],\n",
    "                        batch_size=4096,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(X_val, X_val),\n",
    "                        verbose=self.verbose,\n",
    "                        callbacks=[es,terminate_on_nan,reduce_lr2])\n",
    "        \n",
    "        K.set_value(autoencoder.optimizer.lr, self.lr*2)\n",
    "        \n",
    "        X_train = pd.concat((X1,X2,X3),axis=0)\n",
    "        autoencoder.fit(X_train, X_train,\n",
    "                        epochs=5,\n",
    "                        batch_size=1024,\n",
    "                        shuffle=True,\n",
    "                        verbose=self.verbose,\n",
    "                        callbacks=[es,terminate_on_nan,reduce_lr2])\n",
    "        \n",
    "        \n",
    "        gc.collect()\n",
    "        self.model = encoder\n",
    "        return self\n",
    "\n",
    "    def partial_fit(self, X, y):\n",
    "        return self.fit(X, y)\n",
    "\n",
    "    def transform(self, X, y='ignored'):\n",
    "        if self.store_transform_data:\n",
    "            self.transform_data = (X.copy())\n",
    "        Inp = X\n",
    "        cols = self.cols\n",
    "        Inp = Inp[cols]\n",
    "        Inp = self.imp.transform(Inp)\n",
    "        Inp = self.imp_inf.transform(Inp)\n",
    "        if self.scale_input:\n",
    "            Inp = self.scaler.transform(Inp)\n",
    "        self.check_null_(Inp)\n",
    "        \n",
    "        \n",
    "        results = self.model.predict(Inp)\n",
    "        results = pd.DataFrame(results,columns = list(map(lambda x: self.prefix + str(x), range(0, results.shape[1]))))\n",
    "        results.index = X.index\n",
    "        if not self.inplace:\n",
    "            X = X.copy()\n",
    "        X[results.columns] = results\n",
    "        gc.collect()\n",
    "        return X\n",
    "    \n",
    "\n",
    "    def inverse_transform(self, X, copy=None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def fit_transform(self, X, y=None, sample_weight=None):\n",
    "        self.fit(X, y, sample_weight=sample_weight)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:24:35.281652Z",
     "start_time": "2019-03-20T20:21:38.571736Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "24884/24884 [==============================] - 8s 335us/step - loss: 0.0016\n",
      "Epoch 2/3\n",
      "24884/24884 [==============================] - 1s 42us/step - loss: 9.5326e-04\n",
      "Epoch 3/3\n",
      "24884/24884 [==============================] - 1s 43us/step - loss: 8.1677e-04\n",
      "Train on 16423 samples, validate on 8461 samples\n",
      "Epoch 1/150\n",
      "16423/16423 [==============================] - 4s 220us/step - loss: 7.8583e-04 - val_loss: 7.6592e-04\n",
      "Epoch 2/150\n",
      "16423/16423 [==============================] - 1s 54us/step - loss: 7.8457e-04 - val_loss: 7.6971e-04\n",
      "Epoch 3/150\n",
      "16423/16423 [==============================] - 1s 53us/step - loss: 7.8510e-04 - val_loss: 7.7090e-04\n",
      "Epoch 4/150\n",
      "16423/16423 [==============================] - 1s 61us/step - loss: 7.8527e-04 - val_loss: 7.7124e-04\n",
      "Epoch 5/150\n",
      "16423/16423 [==============================] - 1s 43us/step - loss: 7.8180e-04 - val_loss: 7.6715e-04\n",
      "Epoch 6/150\n",
      "16423/16423 [==============================] - 1s 43us/step - loss: 7.7703e-04 - val_loss: 7.6329e-04\n",
      "Epoch 7/150\n",
      "16423/16423 [==============================] - 1s 45us/step - loss: 7.7356e-04 - val_loss: 7.5989e-04\n",
      "Epoch 8/150\n",
      "16423/16423 [==============================] - 1s 46us/step - loss: 7.7057e-04 - val_loss: 7.5798e-04\n",
      "Epoch 9/150\n",
      "16423/16423 [==============================] - 1s 46us/step - loss: 7.6829e-04 - val_loss: 7.5595e-04\n",
      "Epoch 10/150\n",
      "16423/16423 [==============================] - 1s 50us/step - loss: 7.6614e-04 - val_loss: 7.5365e-04\n",
      "Epoch 11/150\n",
      "16423/16423 [==============================] - 1s 48us/step - loss: 7.6360e-04 - val_loss: 7.5174e-04\n",
      "Epoch 12/150\n",
      "16423/16423 [==============================] - 1s 44us/step - loss: 7.6173e-04 - val_loss: 7.5021e-04\n",
      "Epoch 13/150\n",
      "16423/16423 [==============================] - 1s 49us/step - loss: 7.6017e-04 - val_loss: 7.4905e-04\n",
      "Epoch 14/150\n",
      "16423/16423 [==============================] - 1s 47us/step - loss: 7.5887e-04 - val_loss: 7.4811e-04\n",
      "Epoch 15/150\n",
      "16423/16423 [==============================] - 1s 46us/step - loss: 7.5782e-04 - val_loss: 7.4775e-04\n",
      "Epoch 16/150\n",
      "16423/16423 [==============================] - 1s 48us/step - loss: 7.5644e-04 - val_loss: 7.4614e-04\n",
      "Epoch 17/150\n",
      "16423/16423 [==============================] - 1s 45us/step - loss: 7.5482e-04 - val_loss: 7.4460e-04\n",
      "Epoch 18/150\n",
      "16423/16423 [==============================] - 1s 41us/step - loss: 7.5333e-04 - val_loss: 7.4342e-04\n",
      "Epoch 19/150\n",
      "16423/16423 [==============================] - 1s 42us/step - loss: 7.5230e-04 - val_loss: 7.4218e-04\n",
      "Epoch 20/150\n",
      "16423/16423 [==============================] - 1s 42us/step - loss: 7.5135e-04 - val_loss: 7.4142e-04\n",
      "Epoch 21/150\n",
      "16423/16423 [==============================] - 1s 42us/step - loss: 7.5105e-04 - val_loss: 7.4127e-04\n",
      "Epoch 22/150\n",
      "16423/16423 [==============================] - 1s 47us/step - loss: 7.5036e-04 - val_loss: 7.4043e-04\n",
      "Epoch 23/150\n",
      "16423/16423 [==============================] - 1s 45us/step - loss: 7.4942e-04 - val_loss: 7.3993e-04\n",
      "Epoch 24/150\n",
      "16423/16423 [==============================] - 1s 43us/step - loss: 7.4885e-04 - val_loss: 7.3988e-04\n",
      "Epoch 25/150\n",
      "16423/16423 [==============================] - 1s 42us/step - loss: 7.4859e-04 - val_loss: 7.4058e-04\n",
      "Epoch 26/150\n",
      "16423/16423 [==============================] - 1s 53us/step - loss: 7.4827e-04 - val_loss: 7.3941e-04\n",
      "Epoch 27/150\n",
      "16423/16423 [==============================] - 1s 48us/step - loss: 7.4666e-04 - val_loss: 7.3822e-04\n",
      "Epoch 28/150\n",
      "16423/16423 [==============================] - 1s 47us/step - loss: 7.4559e-04 - val_loss: 7.3756e-04\n",
      "Epoch 29/150\n",
      "16423/16423 [==============================] - 1s 45us/step - loss: 7.4475e-04 - val_loss: 7.3657e-04\n",
      "Epoch 30/150\n",
      "16423/16423 [==============================] - 1s 43us/step - loss: 7.4389e-04 - val_loss: 7.3596e-04\n",
      "Epoch 31/150\n",
      "16423/16423 [==============================] - 1s 43us/step - loss: 7.4330e-04 - val_loss: 7.3555e-04\n",
      "Epoch 32/150\n",
      "16423/16423 [==============================] - 1s 49us/step - loss: 7.4276e-04 - val_loss: 7.3499e-04\n",
      "Epoch 33/150\n",
      "16423/16423 [==============================] - 1s 45us/step - loss: 7.4224e-04 - val_loss: 7.3473e-04\n",
      "Epoch 34/150\n",
      "16423/16423 [==============================] - 1s 45us/step - loss: 7.4199e-04 - val_loss: 7.3436e-04\n",
      "Epoch 35/150\n",
      "16423/16423 [==============================] - 1s 45us/step - loss: 7.4164e-04 - val_loss: 7.3402e-04\n",
      "Epoch 36/150\n",
      "16423/16423 [==============================] - 1s 60us/step - loss: 7.4145e-04 - val_loss: 7.3392e-04\n",
      "Epoch 37/150\n",
      "16423/16423 [==============================] - 1s 57us/step - loss: 7.4121e-04 - val_loss: 7.3378e-04\n",
      "Epoch 38/150\n",
      "16423/16423 [==============================] - 1s 49us/step - loss: 7.4105e-04 - val_loss: 7.3352e-04\n",
      "Epoch 39/150\n",
      "16423/16423 [==============================] - 1s 45us/step - loss: 7.4065e-04 - val_loss: 7.3316e-04\n",
      "Epoch 40/150\n",
      "16423/16423 [==============================] - 1s 45us/step - loss: 7.4028e-04 - val_loss: 7.3290e-04\n",
      "Epoch 41/150\n",
      "16423/16423 [==============================] - 1s 44us/step - loss: 7.4000e-04 - val_loss: 7.3274e-04\n",
      "Epoch 42/150\n",
      "16423/16423 [==============================] - 1s 47us/step - loss: 7.3977e-04 - val_loss: 7.3252e-04\n",
      "Epoch 43/150\n",
      "16423/16423 [==============================] - 1s 46us/step - loss: 7.3955e-04 - val_loss: 7.3234e-04\n",
      "Epoch 44/150\n",
      "16423/16423 [==============================] - 1s 49us/step - loss: 7.3937e-04 - val_loss: 7.3217e-04\n",
      "Epoch 45/150\n",
      "16423/16423 [==============================] - 1s 51us/step - loss: 7.3920e-04 - val_loss: 7.3200e-04\n",
      "Epoch 46/150\n",
      "16423/16423 [==============================] - 1s 45us/step - loss: 7.3901e-04 - val_loss: 7.3185e-04\n",
      "Epoch 47/150\n",
      "16423/16423 [==============================] - 1s 42us/step - loss: 7.3889e-04 - val_loss: 7.3174e-04\n",
      "Epoch 48/150\n",
      "16423/16423 [==============================] - 1s 42us/step - loss: 7.3876e-04 - val_loss: 7.3164e-04\n",
      "Epoch 49/150\n",
      "16423/16423 [==============================] - 1s 45us/step - loss: 7.3864e-04 - val_loss: 7.3155e-04\n",
      "Epoch 50/150\n",
      "16423/16423 [==============================] - 1s 47us/step - loss: 7.3856e-04 - val_loss: 7.3143e-04\n",
      "Epoch 51/150\n",
      "16423/16423 [==============================] - 1s 45us/step - loss: 7.3847e-04 - val_loss: 7.3132e-04\n",
      "Epoch 52/150\n",
      "16423/16423 [==============================] - 1s 41us/step - loss: 7.3840e-04 - val_loss: 7.3127e-04\n",
      "Epoch 53/150\n",
      "16423/16423 [==============================] - 1s 46us/step - loss: 7.3835e-04 - val_loss: 7.3121e-04\n",
      "Epoch 54/150\n",
      "16423/16423 [==============================] - 1s 43us/step - loss: 7.3830e-04 - val_loss: 7.3116e-04\n",
      "Epoch 55/150\n",
      "16423/16423 [==============================] - 1s 46us/step - loss: 7.3826e-04 - val_loss: 7.3114e-04\n",
      "Epoch 56/150\n",
      "16423/16423 [==============================] - 1s 46us/step - loss: 7.3823e-04 - val_loss: 7.3112e-04\n",
      "Epoch 57/150\n",
      "16423/16423 [==============================] - 1s 44us/step - loss: 7.3820e-04 - val_loss: 7.3110e-04\n",
      "Epoch 58/150\n",
      "16423/16423 [==============================] - 1s 49us/step - loss: 7.3818e-04 - val_loss: 7.3109e-04\n",
      "Epoch 59/150\n",
      "16423/16423 [==============================] - 1s 46us/step - loss: 7.3816e-04 - val_loss: 7.3107e-04\n",
      "Epoch 60/150\n",
      "16423/16423 [==============================] - 1s 56us/step - loss: 7.3814e-04 - val_loss: 7.3105e-04\n",
      "Epoch 61/150\n",
      "16423/16423 [==============================] - 1s 46us/step - loss: 7.3812e-04 - val_loss: 7.3104e-04\n",
      "Epoch 62/150\n",
      "16423/16423 [==============================] - 1s 43us/step - loss: 7.3811e-04 - val_loss: 7.3103e-04\n",
      "Epoch 63/150\n",
      "16423/16423 [==============================] - 1s 48us/step - loss: 7.3809e-04 - val_loss: 7.3102e-04\n",
      "Epoch 64/150\n",
      "16423/16423 [==============================] - 1s 45us/step - loss: 7.3808e-04 - val_loss: 7.3100e-04\n",
      "Epoch 65/150\n",
      "16423/16423 [==============================] - 1s 47us/step - loss: 7.3806e-04 - val_loss: 7.3099e-04\n",
      "Epoch 66/150\n",
      "16423/16423 [==============================] - 1s 45us/step - loss: 7.3805e-04 - val_loss: 7.3098e-04\n",
      "Epoch 67/150\n",
      "16423/16423 [==============================] - 1s 47us/step - loss: 7.3803e-04 - val_loss: 7.3097e-04\n",
      "Epoch 68/150\n",
      "16423/16423 [==============================] - 1s 45us/step - loss: 7.3802e-04 - val_loss: 7.3096e-04\n",
      "Epoch 69/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16423/16423 [==============================] - 1s 47us/step - loss: 7.3801e-04 - val_loss: 7.3095e-04\n",
      "Epoch 70/150\n",
      "16423/16423 [==============================] - 1s 44us/step - loss: 7.3799e-04 - val_loss: 7.3094e-04\n",
      "Epoch 71/150\n",
      "16423/16423 [==============================] - 1s 45us/step - loss: 7.3798e-04 - val_loss: 7.3093e-04\n",
      "Epoch 72/150\n",
      "16423/16423 [==============================] - 1s 47us/step - loss: 7.3797e-04 - val_loss: 7.3091e-04\n",
      "Epoch 73/150\n",
      "16423/16423 [==============================] - 1s 46us/step - loss: 7.3796e-04 - val_loss: 7.3090e-04\n",
      "Epoch 74/150\n",
      "16423/16423 [==============================] - 1s 41us/step - loss: 7.3795e-04 - val_loss: 7.3089e-04\n",
      "Epoch 75/150\n",
      "16423/16423 [==============================] - 1s 54us/step - loss: 7.3794e-04 - val_loss: 7.3088e-04\n",
      "Epoch 76/150\n",
      "16423/16423 [==============================] - 1s 52us/step - loss: 7.3793e-04 - val_loss: 7.3087e-04\n",
      "Epoch 77/150\n",
      "16423/16423 [==============================] - 1s 50us/step - loss: 7.3792e-04 - val_loss: 7.3086e-04\n",
      "Epoch 78/150\n",
      "16423/16423 [==============================] - 1s 47us/step - loss: 7.3791e-04 - val_loss: 7.3085e-04\n",
      "Epoch 79/150\n",
      "16423/16423 [==============================] - 1s 47us/step - loss: 7.3789e-04 - val_loss: 7.3084e-04\n",
      "Epoch 80/150\n",
      "16423/16423 [==============================] - 1s 48us/step - loss: 7.3788e-04 - val_loss: 7.3083e-04\n",
      "Epoch 81/150\n",
      "16423/16423 [==============================] - 1s 45us/step - loss: 7.3788e-04 - val_loss: 7.3082e-04\n",
      "Epoch 82/150\n",
      "16423/16423 [==============================] - 1s 46us/step - loss: 7.3787e-04 - val_loss: 7.3081e-04\n",
      "Epoch 83/150\n",
      "16423/16423 [==============================] - 1s 45us/step - loss: 7.3786e-04 - val_loss: 7.3080e-04\n",
      "Epoch 84/150\n",
      "16423/16423 [==============================] - 1s 42us/step - loss: 7.3785e-04 - val_loss: 7.3080e-04\n",
      "Epoch 85/150\n",
      "16423/16423 [==============================] - 1s 45us/step - loss: 7.3784e-04 - val_loss: 7.3079e-04\n",
      "Epoch 86/150\n",
      "16423/16423 [==============================] - 1s 46us/step - loss: 7.3783e-04 - val_loss: 7.3078e-04\n",
      "Epoch 87/150\n",
      "16423/16423 [==============================] - 1s 46us/step - loss: 7.3782e-04 - val_loss: 7.3077e-04\n",
      "Epoch 88/150\n",
      "16423/16423 [==============================] - 1s 46us/step - loss: 7.3781e-04 - val_loss: 7.3076e-04\n",
      "Epoch 89/150\n",
      "16423/16423 [==============================] - 1s 51us/step - loss: 7.3780e-04 - val_loss: 7.3075e-04\n",
      "Epoch 90/150\n",
      "16423/16423 [==============================] - 1s 55us/step - loss: 7.3779e-04 - val_loss: 7.3074e-04\n",
      "Epoch 91/150\n",
      "16423/16423 [==============================] - 1s 56us/step - loss: 7.3778e-04 - val_loss: 7.3074e-04\n",
      "Epoch 92/150\n",
      "16423/16423 [==============================] - 1s 56us/step - loss: 7.3777e-04 - val_loss: 7.3073e-04\n",
      "Epoch 93/150\n",
      "16423/16423 [==============================] - 1s 48us/step - loss: 7.3777e-04 - val_loss: 7.3072e-04\n",
      "Epoch 94/150\n",
      "16423/16423 [==============================] - 1s 45us/step - loss: 7.3776e-04 - val_loss: 7.3071e-04\n",
      "Epoch 95/150\n",
      "16423/16423 [==============================] - 1s 50us/step - loss: 7.3775e-04 - val_loss: 7.3070e-04\n",
      "Epoch 96/150\n",
      "16423/16423 [==============================] - 1s 54us/step - loss: 7.3774e-04 - val_loss: 7.3069e-04\n",
      "Epoch 97/150\n",
      "16423/16423 [==============================] - 1s 59us/step - loss: 7.3773e-04 - val_loss: 7.3068e-04\n",
      "Epoch 98/150\n",
      "16423/16423 [==============================] - 1s 52us/step - loss: 7.3772e-04 - val_loss: 7.3067e-04\n",
      "Epoch 99/150\n",
      "16423/16423 [==============================] - 1s 49us/step - loss: 7.3771e-04 - val_loss: 7.3066e-04\n",
      "Epoch 100/150\n",
      "16423/16423 [==============================] - 1s 54us/step - loss: 7.3770e-04 - val_loss: 7.3066e-04\n",
      "Epoch 101/150\n",
      "16423/16423 [==============================] - 1s 47us/step - loss: 7.3770e-04 - val_loss: 7.3065e-04\n",
      "Epoch 102/150\n",
      "16423/16423 [==============================] - 1s 47us/step - loss: 7.3769e-04 - val_loss: 7.3064e-04\n",
      "Epoch 103/150\n",
      "16423/16423 [==============================] - 1s 49us/step - loss: 7.3768e-04 - val_loss: 7.3064e-04\n",
      "Epoch 104/150\n",
      "16423/16423 [==============================] - 1s 47us/step - loss: 7.3767e-04 - val_loss: 7.3063e-04\n",
      "Epoch 105/150\n",
      "16423/16423 [==============================] - 1s 50us/step - loss: 7.3766e-04 - val_loss: 7.3063e-04\n",
      "Epoch 106/150\n",
      "16423/16423 [==============================] - 1s 47us/step - loss: 7.3766e-04 - val_loss: 7.3062e-04\n",
      "Epoch 107/150\n",
      "16423/16423 [==============================] - 1s 44us/step - loss: 7.3765e-04 - val_loss: 7.3062e-04\n",
      "Epoch 108/150\n",
      "16423/16423 [==============================] - 1s 48us/step - loss: 7.3764e-04 - val_loss: 7.3061e-04\n",
      "Epoch 109/150\n",
      "16423/16423 [==============================] - 1s 46us/step - loss: 7.3763e-04 - val_loss: 7.3061e-04\n",
      "Epoch 110/150\n",
      "16423/16423 [==============================] - 1s 48us/step - loss: 7.3763e-04 - val_loss: 7.3060e-04\n",
      "Epoch 111/150\n",
      "16423/16423 [==============================] - 1s 45us/step - loss: 7.3762e-04 - val_loss: 7.3059e-04\n",
      "Epoch 112/150\n",
      "16423/16423 [==============================] - 1s 49us/step - loss: 7.3761e-04 - val_loss: 7.3059e-04\n",
      "Epoch 113/150\n",
      "16423/16423 [==============================] - 1s 46us/step - loss: 7.3761e-04 - val_loss: 7.3058e-04\n",
      "Epoch 114/150\n",
      "16423/16423 [==============================] - 1s 50us/step - loss: 7.3760e-04 - val_loss: 7.3058e-04\n",
      "Epoch 115/150\n",
      "16423/16423 [==============================] - 1s 45us/step - loss: 7.3759e-04 - val_loss: 7.3057e-04\n",
      "Epoch 116/150\n",
      "16423/16423 [==============================] - 1s 50us/step - loss: 7.3758e-04 - val_loss: 7.3057e-04\n",
      "Epoch 117/150\n",
      "16423/16423 [==============================] - 1s 49us/step - loss: 7.3758e-04 - val_loss: 7.3056e-04\n",
      "Epoch 118/150\n",
      "16423/16423 [==============================] - 1s 44us/step - loss: 7.3757e-04 - val_loss: 7.3055e-04\n",
      "Epoch 119/150\n",
      "16423/16423 [==============================] - 1s 43us/step - loss: 7.3757e-04 - val_loss: 7.3055e-04\n",
      "Epoch 120/150\n",
      "16423/16423 [==============================] - 1s 43us/step - loss: 7.3756e-04 - val_loss: 7.3054e-04\n",
      "Epoch 121/150\n",
      "16423/16423 [==============================] - 1s 49us/step - loss: 7.3756e-04 - val_loss: 7.3053e-04\n",
      "Epoch 122/150\n",
      "16423/16423 [==============================] - 1s 46us/step - loss: 7.3755e-04 - val_loss: 7.3053e-04\n",
      "Epoch 123/150\n",
      "16423/16423 [==============================] - 1s 46us/step - loss: 7.3754e-04 - val_loss: 7.3052e-04\n",
      "Epoch 124/150\n",
      "16423/16423 [==============================] - 1s 46us/step - loss: 7.3754e-04 - val_loss: 7.3051e-04\n",
      "Epoch 125/150\n",
      "16423/16423 [==============================] - 1s 48us/step - loss: 7.3753e-04 - val_loss: 7.3051e-04\n",
      "Epoch 126/150\n",
      "16423/16423 [==============================] - 1s 49us/step - loss: 7.3752e-04 - val_loss: 7.3050e-04\n",
      "Epoch 127/150\n",
      "16423/16423 [==============================] - 1s 49us/step - loss: 7.3752e-04 - val_loss: 7.3050e-04\n",
      "Epoch 128/150\n",
      "16423/16423 [==============================] - 1s 49us/step - loss: 7.3751e-04 - val_loss: 7.3049e-04\n",
      "Epoch 129/150\n",
      "16423/16423 [==============================] - 1s 48us/step - loss: 7.3750e-04 - val_loss: 7.3049e-04\n",
      "Epoch 130/150\n",
      "16423/16423 [==============================] - 1s 59us/step - loss: 7.3750e-04 - val_loss: 7.3048e-04\n",
      "Epoch 131/150\n",
      "16423/16423 [==============================] - 1s 59us/step - loss: 7.3749e-04 - val_loss: 7.3047e-04\n",
      "Epoch 132/150\n",
      "16423/16423 [==============================] - 1s 54us/step - loss: 7.3749e-04 - val_loss: 7.3047e-04\n",
      "Epoch 133/150\n",
      "16423/16423 [==============================] - 1s 52us/step - loss: 7.3748e-04 - val_loss: 7.3046e-04\n",
      "Epoch 134/150\n",
      "16423/16423 [==============================] - 1s 49us/step - loss: 7.3747e-04 - val_loss: 7.3045e-04\n",
      "Epoch 135/150\n",
      "16423/16423 [==============================] - 1s 47us/step - loss: 7.3747e-04 - val_loss: 7.3045e-04\n",
      "Epoch 136/150\n",
      "16423/16423 [==============================] - 1s 49us/step - loss: 7.3746e-04 - val_loss: 7.3044e-04\n",
      "Epoch 137/150\n",
      "16423/16423 [==============================] - 1s 51us/step - loss: 7.3746e-04 - val_loss: 7.3044e-04\n",
      "Epoch 138/150\n",
      "16423/16423 [==============================] - 1s 63us/step - loss: 7.3745e-04 - val_loss: 7.3043e-04\n",
      "Epoch 139/150\n",
      "16423/16423 [==============================] - 1s 54us/step - loss: 7.3744e-04 - val_loss: 7.3043e-04\n",
      "Epoch 140/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16423/16423 [==============================] - 1s 53us/step - loss: 7.3744e-04 - val_loss: 7.3043e-04\n",
      "Epoch 141/150\n",
      "16423/16423 [==============================] - 1s 48us/step - loss: 7.3743e-04 - val_loss: 7.3042e-04\n",
      "Epoch 142/150\n",
      "16423/16423 [==============================] - 1s 44us/step - loss: 7.3743e-04 - val_loss: 7.3042e-04\n",
      "Epoch 143/150\n",
      "16423/16423 [==============================] - 1s 44us/step - loss: 7.3742e-04 - val_loss: 7.3042e-04\n",
      "Epoch 144/150\n",
      "16423/16423 [==============================] - 1s 45us/step - loss: 7.3742e-04 - val_loss: 7.3041e-04\n",
      "Epoch 145/150\n",
      "16423/16423 [==============================] - 1s 53us/step - loss: 7.3741e-04 - val_loss: 7.3041e-04\n",
      "Epoch 146/150\n",
      "16423/16423 [==============================] - 1s 47us/step - loss: 7.3741e-04 - val_loss: 7.3041e-04\n",
      "Epoch 147/150\n",
      "16423/16423 [==============================] - 1s 46us/step - loss: 7.3740e-04 - val_loss: 7.3040e-04\n",
      "Epoch 148/150\n",
      "16423/16423 [==============================] - 1s 48us/step - loss: 7.3740e-04 - val_loss: 7.3040e-04\n",
      "Epoch 149/150\n",
      "16423/16423 [==============================] - 1s 47us/step - loss: 7.3740e-04 - val_loss: 7.3039e-04\n",
      "Epoch 150/150\n",
      "16423/16423 [==============================] - 1s 52us/step - loss: 7.3739e-04 - val_loss: 7.3039e-04\n",
      "LR:  5e-05\n",
      "Train on 16673 samples, validate on 8211 samples\n",
      "Epoch 1/50\n",
      "16673/16673 [==============================] - 1s 51us/step - loss: 7.2543e-04 - val_loss: 7.5532e-04\n",
      "Epoch 2/50\n",
      "16673/16673 [==============================] - 1s 50us/step - loss: 7.2556e-04 - val_loss: 7.5546e-04\n",
      "Epoch 3/50\n",
      "16673/16673 [==============================] - 1s 50us/step - loss: 7.2486e-04 - val_loss: 7.5554e-04\n",
      "Epoch 4/50\n",
      "16673/16673 [==============================] - 1s 49us/step - loss: 7.2417e-04 - val_loss: 7.5549e-04\n",
      "Epoch 5/50\n",
      "16673/16673 [==============================] - 1s 53us/step - loss: 7.2369e-04 - val_loss: 7.5540e-04\n",
      "Epoch 6/50\n",
      "16673/16673 [==============================] - 1s 46us/step - loss: 7.2324e-04 - val_loss: 7.5536e-04\n",
      "Epoch 7/50\n",
      "16673/16673 [==============================] - 1s 48us/step - loss: 7.2300e-04 - val_loss: 7.5533e-04\n",
      "Epoch 8/50\n",
      "16673/16673 [==============================] - 1s 45us/step - loss: 7.2280e-04 - val_loss: 7.5532e-04\n",
      "Epoch 9/50\n",
      "16673/16673 [==============================] - 1s 51us/step - loss: 7.2269e-04 - val_loss: 7.5530e-04\n",
      "Epoch 10/50\n",
      "16673/16673 [==============================] - 1s 47us/step - loss: 7.2260e-04 - val_loss: 7.5530e-04\n",
      "Epoch 11/50\n",
      "16673/16673 [==============================] - 1s 52us/step - loss: 7.2255e-04 - val_loss: 7.5529e-04\n",
      "Epoch 12/50\n",
      "16673/16673 [==============================] - 1s 47us/step - loss: 7.2250e-04 - val_loss: 7.5529e-04\n",
      "Epoch 13/50\n",
      "16673/16673 [==============================] - 1s 47us/step - loss: 7.2248e-04 - val_loss: 7.5529e-04\n",
      "Epoch 14/50\n",
      "16673/16673 [==============================] - 1s 45us/step - loss: 7.2245e-04 - val_loss: 7.5528e-04\n",
      "Epoch 15/50\n",
      "16673/16673 [==============================] - 1s 52us/step - loss: 7.2244e-04 - val_loss: 7.5528e-04\n",
      "Epoch 16/50\n",
      "16673/16673 [==============================] - 1s 47us/step - loss: 7.2243e-04 - val_loss: 7.5528e-04\n",
      "Epoch 17/50\n",
      "16673/16673 [==============================] - 1s 45us/step - loss: 7.2242e-04 - val_loss: 7.5528e-04\n",
      "Epoch 18/50\n",
      "16673/16673 [==============================] - 1s 44us/step - loss: 7.2241e-04 - val_loss: 7.5528e-04\n",
      "Epoch 19/50\n",
      "16673/16673 [==============================] - 1s 44us/step - loss: 7.2240e-04 - val_loss: 7.5528e-04\n",
      "Epoch 20/50\n",
      "16673/16673 [==============================] - 1s 44us/step - loss: 7.2240e-04 - val_loss: 7.5528e-04\n",
      "Epoch 21/50\n",
      "16673/16673 [==============================] - 1s 44us/step - loss: 7.2239e-04 - val_loss: 7.5528e-04\n",
      "Epoch 22/50\n",
      "16673/16673 [==============================] - 1s 43us/step - loss: 7.2238e-04 - val_loss: 7.5527e-04\n",
      "Epoch 23/50\n",
      "16673/16673 [==============================] - 1s 43us/step - loss: 7.2237e-04 - val_loss: 7.5527e-04\n",
      "Epoch 24/50\n",
      "16673/16673 [==============================] - 1s 44us/step - loss: 7.2236e-04 - val_loss: 7.5527e-04\n",
      "Epoch 1/2\n",
      "24884/24884 [==============================] - 1s 43us/step - loss: 7.3343e-04\n",
      "Epoch 2/2\n",
      "24884/24884 [==============================] - 1s 43us/step - loss: 7.3163e-04\n",
      "LR:  0.01\n",
      "Post value set LR:  0.01\n",
      "Train on 16672 samples, validate on 8212 samples\n",
      "Epoch 1/25\n",
      "16672/16672 [==============================] - 1s 45us/step - loss: 7.3631e-04 - val_loss: 7.1585e-04\n",
      "Epoch 2/25\n",
      "16672/16672 [==============================] - 1s 43us/step - loss: 7.3588e-04 - val_loss: 7.1639e-04\n",
      "Epoch 3/25\n",
      "16672/16672 [==============================] - 1s 43us/step - loss: 7.3549e-04 - val_loss: 7.1699e-04\n",
      "Epoch 4/25\n",
      "16672/16672 [==============================] - 1s 44us/step - loss: 7.3507e-04 - val_loss: 7.1708e-04\n",
      "Epoch 5/25\n",
      "16672/16672 [==============================] - 1s 43us/step - loss: 7.3475e-04 - val_loss: 7.1719e-04\n",
      "Epoch 6/25\n",
      "16672/16672 [==============================] - 1s 43us/step - loss: 7.3450e-04 - val_loss: 7.1722e-04\n",
      "Epoch 7/25\n",
      "16672/16672 [==============================] - 1s 43us/step - loss: 7.3432e-04 - val_loss: 7.1724e-04\n",
      "Epoch 8/25\n",
      "16672/16672 [==============================] - 1s 45us/step - loss: 7.3417e-04 - val_loss: 7.1724e-04\n",
      "Epoch 9/25\n",
      "16672/16672 [==============================] - 1s 43us/step - loss: 7.3409e-04 - val_loss: 7.1723e-04\n",
      "Epoch 1/5\n",
      "24884/24884 [==============================] - 1s 43us/step - loss: 7.3049e-04\n",
      "Epoch 2/5\n",
      "24884/24884 [==============================] - 1s 44us/step - loss: 7.2936e-04\n",
      "Epoch 3/5\n",
      "24884/24884 [==============================] - 1s 44us/step - loss: 7.2825e-04\n",
      "Epoch 4/5\n",
      "24884/24884 [==============================] - 1s 44us/step - loss: 7.2742e-04\n",
      "Epoch 5/5\n",
      "24884/24884 [==============================] - 1s 44us/step - loss: 7.2665e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.AutoEncoderKerasDNN at 0x1aa42a9c88>"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "autoenc = AutoEncoderKerasDNN(columns=features_useful,n_components=64,verbose=True)\n",
    "\n",
    "autoenc.fit(pd.concat((X,X_test),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:26:00.373499Z",
     "start_time": "2019-03-20T20:25:53.805301Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1_score_weighted</th>\n",
       "      <td>0.985598</td>\n",
       "      <td>0.679761</td>\n",
       "      <td>0.305837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score_macro</th>\n",
       "      <td>0.990673</td>\n",
       "      <td>0.645034</td>\n",
       "      <td>0.345639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.985620</td>\n",
       "      <td>0.682413</td>\n",
       "      <td>0.303207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      train      test      diff\n",
       "f1_score_weighted  0.985598  0.679761  0.305837\n",
       "f1_score_macro     0.990673  0.645034  0.345639\n",
       "accuracy           0.985620  0.682413  0.303207"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = autoenc.transform(X)\n",
    "X_train = X_train[df_utils.get_specific_cols(X_train,prefix=\"autoenc_\")]\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=10,learning_rate=0.6,gamma=0,\n",
    "                               missing=np.NaN,max_depth=4,n_jobs=int(multiprocessing.cpu_count()),objective='multi:softmax')\n",
    "    return classifier2\n",
    "\n",
    "cross_validate_classifier(build_model,X_train,y,y_ohe,score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:26:09.563773Z",
     "start_time": "2019-03-20T20:26:06.582511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1_score_weighted</th>\n",
       "      <td>0.600070</td>\n",
       "      <td>0.468770</td>\n",
       "      <td>0.131300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score_macro</th>\n",
       "      <td>0.605731</td>\n",
       "      <td>0.425913</td>\n",
       "      <td>0.179818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.596727</td>\n",
       "      <td>0.471987</td>\n",
       "      <td>0.124740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      train      test      diff\n",
       "f1_score_weighted  0.600070  0.468770  0.131300\n",
       "f1_score_macro     0.605731  0.425913  0.179818\n",
       "accuracy           0.596727  0.471987  0.124740"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "def build_model_rb(scale_pos_weight=None):\n",
    "    classifier = RUSBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=4),n_estimators=15, \n",
    "                                    learning_rate=0.4, algorithm='SAMME.R', \n",
    "                                    sampling_strategy='auto', replacement=False)\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "cross_validate_classifier(build_model_rb,X_train,y,y_ohe,score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T19:41:23.998621Z",
     "start_time": "2019-03-20T19:41:23.964594Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from data_science_utils.dataframe import get_specific_cols\n",
    "\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import BaseWrapper\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import TerminateOnNaN\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "class ClassifierKerasDNN:\n",
    "    def __init__(self, network_config,lr=0.005,\n",
    "                 n_iter=[50,25,15], columns=[], prefixes=None, suffixes=None,\n",
    "                 store_train_data=False,\n",
    "                 store_transform_data=False,\n",
    "                 scale_input=True, impute=True, raise_null=True,verbose=True,):\n",
    "        self.network_config = network_config\n",
    "        self.columns = columns\n",
    "        self.prefixes = prefixes\n",
    "        self.suffixes = suffixes\n",
    "        assert len(columns) > 0 or prefixes is not None\n",
    "        self.scale_input = scale_input\n",
    "        self.scaler = StandardScaler()\n",
    "        self.impute = impute\n",
    "        self.imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        self.imp_inf = SimpleImputer(missing_values=np.inf, strategy='mean')\n",
    "        self.raise_null = raise_null\n",
    "        self.cols = None\n",
    "        self.train = None\n",
    "        self.store_train_data = store_train_data\n",
    "        self.store_transform_data = store_transform_data\n",
    "        self.transform_data = None\n",
    "        self.n_iter = n_iter\n",
    "        self.verbose = verbose\n",
    "        self.lr = lr\n",
    "\n",
    "    def check_null_(self, X):\n",
    "        nans = np.isnan(X)\n",
    "        infs = np.isinf(X)\n",
    "        nan_summary = np.sum(np.logical_or(nans, infs))\n",
    "        if nan_summary > 0:\n",
    "            raise ValueError(\"nans/inf in frame = %s\" % (nan_summary))\n",
    "\n",
    "    def get_cols_(self, X):\n",
    "        cols = list(self.columns)\n",
    "        if self.prefixes is not None:\n",
    "            for pf in self.prefixes:\n",
    "                cols.extend(get_specific_cols(X, prefix=pf))\n",
    "        if self.suffixes is not None:\n",
    "            for pf in self.suffixes:\n",
    "                cols.extend(get_specific_cols(X, suffix=pf))\n",
    "        cols = list(set(cols))\n",
    "        return cols\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        if self.store_train_data:\n",
    "            self.train = (X.copy(),y.copy(),sample_weight)\n",
    "        cols = self.get_cols_(X)\n",
    "        self.cols = cols\n",
    "        X = X[cols]\n",
    "        if self.impute:\n",
    "            X = self.imp.fit_transform(X)\n",
    "            X = self.imp_inf.fit_transform(X)\n",
    "        if self.scale_input:\n",
    "            X = self.scaler.fit_transform(X)\n",
    "        if self.raise_null:\n",
    "            self.check_null_(X)\n",
    "            \n",
    "        y = to_categorical(y)\n",
    "            \n",
    "        model = Sequential()\n",
    "        i = 0\n",
    "        for layer in self.network_config:\n",
    "            if i==0:\n",
    "                model.add(Dense(layer['neurons'],activation=layer['activation'],input_dim=X.shape[1],use_bias=True))\n",
    "                if \"dropout\" in layer:\n",
    "                    model.add(Dropout(layer[\"dropout\"]))\n",
    "                else:\n",
    "                    model.add(Dropout(0.2))\n",
    "            else:\n",
    "                model.add(Dense(layer['neurons'],activation=layer['activation'],use_bias=True))\n",
    "                if \"dropout\" in layer:\n",
    "                    model.add(Dropout(layer[\"dropout\"]))\n",
    "                else:\n",
    "                    model.add(Dropout(0.1))\n",
    "            i=i+1\n",
    "        model.add(Dense(10, kernel_initializer='normal', activation='softmax'))\n",
    "        \n",
    "        adam = optimizers.Adam(lr=self.lr, clipnorm=2, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.05, amsgrad=False)\n",
    "        model.compile(optimizer=adam, loss='categorical_crossentropy')\n",
    "        \n",
    "        \n",
    "        X1,X2,X3 = np.split(X, [int(.33*len(X)), int(.66*len(X))])\n",
    "        y1,y2,y3 = np.split(y, [int(.33*len(y)), int(.66*len(y))])\n",
    "        \n",
    "        X1,X2,X3 = pd.DataFrame(X1),pd.DataFrame(X2),pd.DataFrame(X3)\n",
    "        \n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=5, min_lr=0.00005,epsilon=0.0001)\n",
    "        reduce_lr2 = ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=3, min_lr=0.00005,epsilon=0.0001)\n",
    "        terminate_on_nan = TerminateOnNaN()\n",
    "        es = EarlyStopping(monitor='val_loss', min_delta=0.000001, patience=12, verbose=0,)\n",
    "        es2 = EarlyStopping(monitor='val_loss', min_delta=0.000001, patience=6, verbose=0,)\n",
    "        # print(K.get_value(model.optimizer.lr))\n",
    "        X_train,y_train,X_val,y_val = pd.concat((X1,X2),axis=0),np.concatenate((y1,y2),axis=0),X3,y3\n",
    "        model.fit(X_train, y_train,\n",
    "                        epochs=self.n_iter[0],\n",
    "                        batch_size=256,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        verbose=self.verbose,\n",
    "                        callbacks=[es,terminate_on_nan,reduce_lr])\n",
    "        # K.set_value(model.optimizer.lr, self.lr/2)\n",
    "        # print(K.get_value(model.optimizer.lr))\n",
    "        \n",
    "        X_train,y_train,X_val,y_val = pd.concat((X2,X3),axis=0),np.concatenate((y2,y3),axis=0),X1,y1\n",
    "        model.fit(X_train, y_train,\n",
    "                        epochs=self.n_iter[1],\n",
    "                        batch_size=256,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        verbose=self.verbose,\n",
    "                        callbacks=[es2,terminate_on_nan,reduce_lr2])\n",
    "        \n",
    "        \n",
    "        K.set_value(model.optimizer.lr, self.lr/10)\n",
    "        # print(K.get_value(model.optimizer.lr))\n",
    "        \n",
    "        X_train,y_train,X_val,y_val = pd.concat((X1,X3),axis=0),np.concatenate((y1,y3),axis=0),X2,y2\n",
    "        model.fit(X_train, y_train,\n",
    "                        epochs=self.n_iter[2],\n",
    "                        batch_size=256,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        verbose=self.verbose,\n",
    "                        callbacks=[es2,terminate_on_nan,reduce_lr2])\n",
    "        # print(K.get_value(model.optimizer.lr))\n",
    "        \n",
    "        \n",
    "        gc.collect()\n",
    "        self.classifier = model\n",
    "        return self\n",
    "\n",
    "    def partial_fit(self, X, y):\n",
    "        return self.fit(X, y)\n",
    "\n",
    "    def predict_proba(self, X, y='ignored'):\n",
    "        if self.store_transform_data:\n",
    "            self.transform_data = (X.copy())\n",
    "        Inp = X\n",
    "        cols = self.cols\n",
    "        Inp = Inp[cols]\n",
    "        if self.impute:\n",
    "            Inp = self.imp.transform(Inp)\n",
    "            Inp = self.imp_inf.transform(Inp)\n",
    "        if self.scale_input:\n",
    "            Inp = self.scaler.transform(Inp)\n",
    "\n",
    "        if self.raise_null:\n",
    "            self.check_null_(Inp)\n",
    "        probas = self.classifier.predict(Inp)\n",
    "        gc.collect()\n",
    "        return probas\n",
    "    \n",
    "    def predict(self,X,y='ignored'):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "    def inverse_transform(self, X, copy=None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def fit_transform(self, X, y, sample_weight=None):\n",
    "        self.fit(X, y, sample_weight=sample_weight)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:34:29.097888Z",
     "start_time": "2019-03-20T20:34:29.093243Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    network_config = [{'neurons':128,'activation':'elu',\"dropout\":0.3},\n",
    "                         {'neurons':128,'activation':'elu',\"dropout\":0.2},\n",
    "                          {'neurons':96,'activation':'elu',\"dropout\":0.1},\n",
    "                         {'neurons':96,'activation':'selu',\"dropout\":0}]\n",
    "    nn = ClassifierKerasDNN(network_config,lr=0.005,prefixes=[\"autoenc_\"],\n",
    "                                            store_transform_data=False,store_train_data=False,verbose=False)\n",
    "    return nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:38:58.833215Z",
     "start_time": "2019-03-20T20:34:35.839589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1_score_weighted</th>\n",
       "      <td>0.827696</td>\n",
       "      <td>0.752384</td>\n",
       "      <td>0.075312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score_macro</th>\n",
       "      <td>0.834261</td>\n",
       "      <td>0.749840</td>\n",
       "      <td>0.084422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.832709</td>\n",
       "      <td>0.761206</td>\n",
       "      <td>0.071502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      train      test      diff\n",
       "f1_score_weighted  0.827696  0.752384  0.075312\n",
       "f1_score_macro     0.834261  0.749840  0.084422\n",
       "accuracy           0.832709  0.761206  0.071502"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "cross_validate_classifier(build_model,X_train,y,y_ohe,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:03:52.081292Z",
     "start_time": "2019-03-20T20:03:30.163875Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ClassifierKerasDNN at 0x1a74608908>"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.fit(X_train,y)\n",
    "y_pred = model.predict(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T19:35:36.826171Z",
     "start_time": "2019-03-20T19:35:36.807940Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9    363\n",
       "0    314\n",
       "8    168\n",
       "7     81\n",
       "6     69\n",
       "5     66\n",
       "4     65\n",
       "3     46\n",
       "2     36\n",
       "1     36\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th>predictions</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">0</th>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">6</th>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">8</th>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">9</th>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    index\n",
       "actual predictions       \n",
       "0      1                1\n",
       "       2                2\n",
       "       3                3\n",
       "       4                4\n",
       "       5                2\n",
       "       6                2\n",
       "       7                4\n",
       "       8               44\n",
       "       9               32\n",
       "1      0               13\n",
       "       8                9\n",
       "       9                1\n",
       "3      0                2\n",
       "       8                3\n",
       "4      0                1\n",
       "5      8                1\n",
       "       9                6\n",
       "6      0                2\n",
       "       9                1\n",
       "7      0                4\n",
       "8      0               33\n",
       "       1                5\n",
       "       3                1\n",
       "       5                3\n",
       "       9                7\n",
       "9      0                7\n",
       "       5                6\n",
       "       6                4"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# disagreements\n",
    "y.value_counts()\n",
    "misclassified = y_pred!=y.values\n",
    "misclassified = pd.DataFrame({\"actual\":y.values[misclassified],\"predictions\":y_pred[misclassified]})\n",
    "misclassified.shape[0]\n",
    "most_common_misclassifications = misclassified.reset_index().groupby([\"actual\",\"predictions\"]).count()\n",
    "most_common_misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:04:09.483594Z",
     "start_time": "2019-03-20T20:04:00.284675Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X_test = autoenc.transform(X_test)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputting data\n",
    "Here is a quick snippet of code to write the data to disk, and then we'll talk how to upload to leaderboard.  I'll make the all zero prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:04:09.561691Z",
     "start_time": "2019-03-20T20:04:09.485796Z"
    }
   },
   "outputs": [],
   "source": [
    "test['label'] = y_pred\n",
    "test[['label']].to_csv('sub6.csv', index=True, index_label = 'ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting our model output out of Eider and into Leaderboard\n",
    "\n",
    "We now have our model's output .csv and are ready to upload to Leaderboard\n",
    "1. Search for your [Leaderboard instance](https://leaderboard.corp.amazon.com/tasks/292) and go to the 'Make a Submission' section\n",
    "2. Upload your local file and include your notebook version URL for tracking.\n",
    "3. Your score on the public leaderboard should now appear. \n",
    "\n",
    "The private leaderboard contains the vast majority of the data, and so your final rankings in this competition will be a bit of a surprise! Take care and avoid overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "688.264px",
    "left": "0px",
    "right": "1382.45px",
    "top": "109.722px",
    "width": "211.997px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
